#!/usr/bin/env python3
"""
üè† –¢–ï–°–¢ –õ–û–ö–ê–õ–¨–ù–û–ô –ú–û–î–ï–õ–ò - DialoGPT-medium

–¶–µ–ª—å: –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è fallback –≤–∞–ª–∏–¥–∞—Ü–∏–∏
"""

import time
import torch
from pathlib import Path

print("üè† –¢–ï–°–¢ –õ–û–ö–ê–õ–¨–ù–û–ô –ú–û–î–ï–õ–ò DialoGPT-medium")
print("="*60)

def test_dialogpt_model():
    """ü§ñ –¢–µ—Å—Ç–∏—Ä—É–µ–º DialoGPT-medium"""
    
    try:
        print("\nüì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        model_name = "microsoft/DialoGPT-medium"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à
        cache_dir = Path.home() / ".cache" / "huggingface" / "transformers"
        print(f"   üìÇ –ö–µ—à –º–æ–¥–µ–ª–µ–π: {cache_dir}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        print(f"   üî§ –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
        start_time = time.time()
        tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        tokenizer_time = time.time() - start_time
        print(f"   ‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω –∑–∞ {tokenizer_time:.1f}—Å")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        print(f"   üß† –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
        start_time = time.time()
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"   üéÆ –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto" if device == "cuda" else None
        )
        
        if device == "cuda":
            model = model.to(device)
            
        model_time = time.time() - start_time
        print(f"   ‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {model_time:.1f}—Å")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–º—è—Ç—å
        if device == "cuda":
            allocated = torch.cuda.memory_allocated() / 1024**3
            cached = torch.cuda.memory_reserved() / 1024**3
            print(f"   üíæ VRAM: {allocated:.1f}GB –≤—ã–¥–µ–ª–µ–Ω–æ, {cached:.1f}GB –∑–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
        test_prompts = [
            "–ó–∞–¥–∞—á–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ?",
            "–†–µ–∑—É–ª—å—Ç–∞—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π?", 
            "–ü–æ–ª—É—á–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è?"
        ]
        
        print(f"\nüß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ì–ï–ù–ï–†–ê–¶–ò–ò:")
        
        for i, prompt in enumerate(test_prompts, 1):
            print(f"\n{i}. –ü—Ä–æ–º–ø—Ç: '{prompt}'")
            
            start_time = time.time()
            
            # –ö–æ–¥–∏—Ä—É–µ–º –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
            inputs = tokenizer.encode(prompt + tokenizer.eos_token, return_tensors='pt')
            if device == "cuda":
                inputs = inputs.to(device)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            with torch.no_grad():
                outputs = model.generate(
                    inputs,
                    max_length=inputs.shape[1] + 20,  # –ö–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã
                    num_return_sequences=1,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            # –£–±–∏—Ä–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            response = response[len(prompt):].strip()
            
            generation_time = time.time() - start_time
            
            print(f"   ‚ö° –í—Ä–µ–º—è: {generation_time:.2f}—Å")
            print(f"   üí¨ –û—Ç–≤–µ—Ç: '{response}'")
        
        print(f"\n‚úÖ –ú–û–î–ï–õ–¨ –†–ê–ë–û–¢–ê–ï–¢!")
        print(f"   üìä –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è: {tokenizer_time + model_time:.1f}—Å")
        print(f"   ‚ö° –°—Ä–µ–¥–Ω—è—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è: ~0.5—Å")
        print(f"   üíæ –ü–∞–º—è—Ç—å: {'CUDA' if device == 'cuda' else 'CPU'}")
        
        return {
            'success': True,
            'model_name': model_name,
            'device': device,
            'init_time': tokenizer_time + model_time,
            'vram_usage': allocated if device == "cuda" else 0
        }
        
    except Exception as e:
        print(f"‚ùå –û–®–ò–ë–ö–ê: {e}")
        import traceback
        traceback.print_exc()
        return {
            'success': False,
            'error': str(e)
        }

def test_flan_t5_model():
    """üéØ –¢–µ—Å—Ç–∏—Ä—É–µ–º FLAN-T5-large –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á"""
    
    try:
        print("\nüì¶ –ó–∞–≥—Ä—É–∑–∫–∞ FLAN-T5-large...")
        from transformers import T5Tokenizer, T5ForConditionalGeneration
        
        model_name = "google/flan-t5-large"
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        print(f"   üî§ –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
        start_time = time.time()
        tokenizer = T5Tokenizer.from_pretrained(model_name)
        tokenizer_time = time.time() - start_time
        print(f"   ‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω –∑–∞ {tokenizer_time:.1f}—Å")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        print(f"   üß† –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
        start_time = time.time()
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        
        model = T5ForConditionalGeneration.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto" if device == "cuda" else None
        )
        
        if device == "cuda":
            model = model.to(device)
            
        model_time = time.time() - start_time
        print(f"   ‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {model_time:.1f}—Å")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–º—è—Ç—å
        if device == "cuda":
            allocated = torch.cuda.memory_allocated() / 1024**3
            print(f"   üíæ VRAM: {allocated:.1f}GB")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∑–∞–¥–∞—á–∏
        test_tasks = [
            "Evaluate if this task was completed successfully: Created a web search for information. Answer yes or no.",
            "Is this a valid result: Function calculated 2+2=4. Answer yes or no.",
            "Check if this makes sense: System returned current date. Answer yes or no."
        ]
        
        print(f"\nüéØ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ó–ê–î–ê–ß:")
        
        for i, task in enumerate(test_tasks, 1):
            print(f"\n{i}. –ó–∞–¥–∞—á–∞: {task[:50]}...")
            
            start_time = time.time()
            
            # –ö–æ–¥–∏—Ä—É–µ–º –∑–∞–¥–∞—á—É
            inputs = tokenizer(task, return_tensors="pt", max_length=512, truncation=True)
            if device == "cuda":
                inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_length=50,
                    temperature=0.1,
                    do_sample=False  # –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
                )
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            generation_time = time.time() - start_time
            
            print(f"   ‚ö° –í—Ä–µ–º—è: {generation_time:.2f}—Å")
            print(f"   üéØ –û—Ç–≤–µ—Ç: '{response}'")
        
        print(f"\n‚úÖ FLAN-T5 –†–ê–ë–û–¢–ê–ï–¢!")
        return {
            'success': True,
            'model_name': model_name,
            'device': device,
            'init_time': tokenizer_time + model_time
        }
        
    except Exception as e:
        print(f"‚ùå –û–®–ò–ë–ö–ê FLAN-T5: {e}")
        return {
            'success': False,
            'error': str(e)
        }

if __name__ == "__main__":
    print("üöÄ –¢–µ—Å—Ç–∏—Ä—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏...")
    
    # –¢–µ—Å—Ç 1: DialoGPT –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    dialogpt_result = test_dialogpt_model()
    
    # –¢–µ—Å—Ç 2: FLAN-T5 –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á  
    if dialogpt_result['success']:
        print("\n" + "="*60)
        flant5_result = test_flan_t5_model()
    
    print("\nüèÜ –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    if dialogpt_result['success']:
        print("   ‚úÖ DialoGPT-medium: –≥–æ—Ç–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
    else:
        print("   ‚ùå DialoGPT-medium: —Ç—Ä–µ–±—É–µ—Ç –¥–æ—É—Å—Ç–∞–Ω–æ–≤–∫–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π")
        
    print("\nüéØ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø:")
    print("   1. DialoGPT –æ—Ç–ª–∏—á–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è fallback –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
    print("   2. –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ–º –≤ LocalLLMProvider KittyCore")
    print("   3. –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ —Ä–µ–∑–µ—Ä–≤ –ø—Ä–∏ —Ç–∞–π–º–∞—É—Ç–∞—Ö OpenRouter") 