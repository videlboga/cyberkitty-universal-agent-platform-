"""
üß† Memory Management Engine - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–∞–º—è—Ç–∏ –¥–ª—è KittyCore

–ü—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –ø–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏:
- ‚úÖ –ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ (–∫–æ—Ä–æ—Ç–∫–∞—è/–¥–æ–ª–≥–∞—è/—Ä–∞–±–æ—á–∞—è –ø–∞–º—è—Ç—å)
- ‚úÖ –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ ChromaDB
- ‚úÖ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å–∂–∞—Ç–∏–µ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π
- ‚úÖ –ö—Ä–æ—Å—Å-–∞–≥–µ–Ω—Ç–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è –∫–æ–º–∞–Ω–¥
- ‚úÖ –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Å—Ä–µ–∑—ã –∏ replay
- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏
- ‚úÖ –ü–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –º–µ–∂–¥—É —Å–µ—Å—Å–∏—è–º–∏

Inspiration: CrewAI + LangGraph + AutoGen, –Ω–æ –õ–£–ß–®–ï! üöÄ
"""

import asyncio
import json
import sqlite3
import hashlib
import logging
from abc import ABC, abstractmethod
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Union, Tuple
from pathlib import Path
import numpy as np

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = logging.getLogger(__name__)

# === –ë–ê–ó–û–í–´–ï –ò–ù–¢–ï–†–§–ï–ô–°–´ ===

@dataclass
class MemoryEntry:
    """–ë–∞–∑–æ–≤–∞—è –∑–∞–ø–∏—Å—å –≤ –ø–∞–º—è—Ç–∏"""
    id: str
    content: str
    metadata: Dict[str, Any]
    timestamp: datetime
    embedding: Optional[List[float]] = None
    access_count: int = 0
    importance_score: float = 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            **asdict(self),
            "timestamp": self.timestamp.isoformat()
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'MemoryEntry':
        data["timestamp"] = datetime.fromisoformat(data["timestamp"])
        return cls(**data)


@dataclass  
class MemorySearchResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ –≤ –ø–∞–º—è—Ç–∏"""
    entry: MemoryEntry
    similarity_score: float
    retrieval_reason: str


class BaseMemoryLayer(ABC):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è —Å–ª–æ—è –ø–∞–º—è—Ç–∏"""
    
    @abstractmethod
    async def store(self, entry: MemoryEntry) -> bool:
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∑–∞–ø–∏—Å—å"""
        pass
    
    @abstractmethod
    async def search(self, query: str, limit: int = 5) -> List[MemorySearchResult]:
        """–ü–æ–∏—Å–∫ –∑–∞–ø–∏—Å–µ–π"""
        pass
    
    @abstractmethod
    async def get(self, entry_id: str) -> Optional[MemoryEntry]:
        """–ü–æ–ª—É—á–∏—Ç—å –∑–∞–ø–∏—Å—å –ø–æ ID"""
        pass
    
    @abstractmethod
    async def delete(self, entry_id: str) -> bool:
        """–£–¥–∞–ª–∏—Ç—å –∑–∞–ø–∏—Å—å"""
        pass
    
    @abstractmethod
    async def clear(self) -> bool:
        """–û—á–∏—Å—Ç–∏—Ç—å —Å–ª–æ–π"""
        pass
    
    @abstractmethod
    async def get_stats(self) -> Dict[str, Any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–ª–æ—è"""
        pass


class MemoryCompressor(ABC):
    """–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Å–∂–∞—Ç–∏—è –ø–∞–º—è—Ç–∏"""
    
    @abstractmethod
    async def compress_memories(self, entries: List[MemoryEntry]) -> List[MemoryEntry]:
        """–°–∂–∞—Ç—å –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è –≤ –±–æ–ª–µ–µ –∫–æ–º–ø–∞–∫—Ç–Ω—É—é —Ñ–æ—Ä–º—É"""
        pass


class MemoryOptimizer(ABC):
    """–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏"""
    
    @abstractmethod
    async def optimize(self, layer: BaseMemoryLayer) -> Dict[str, Any]:
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–ª–æ–π –ø–∞–º—è—Ç–∏"""
        pass


# === –£–¢–ò–õ–ò–¢–´ ===

class MemoryUtils:
    """–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ø–∞–º—è—Ç—å—é"""
    
    @staticmethod
    def generate_id(content: str, metadata: Dict[str, Any] = None) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è –∑–∞–ø–∏—Å–∏"""
        combined = f"{content}:{json.dumps(metadata or {}, sort_keys=True)}"
        return hashlib.sha256(combined.encode()).hexdigest()[:16]
    
    @staticmethod
    def calculate_importance(
        content: str, 
        metadata: Dict[str, Any],
        access_count: int = 0
    ) -> float:
        """–í—ã—á–∏—Å–ª–∏—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –∑–∞–ø–∏—Å–∏ (0.0 - 1.0)"""
        # –ë–∞–∑–æ–≤–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å –æ—Ç –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        content_score = min(1.0, len(content) / 1000)
        
        # –ë–æ–Ω—É—Å –∑–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        keywords = ["–æ—à–∏–±–∫–∞", "–≤–∞–∂–Ω–æ", "–∫—Ä–∏—Ç–∏—á–Ω–æ", "–ø—Ä–æ–±–ª–µ–º–∞", "—Ä–µ—à–µ–Ω–∏–µ"]
        keyword_score = sum(1 for kw in keywords if kw in content.lower()) * 0.1
        
        # –ë–æ–Ω—É—Å –∑–∞ —á–∞—Å—Ç–æ—Ç—É –¥–æ—Å—Ç—É–ø–∞
        access_score = min(0.3, access_count * 0.05)
        
        # –ë–æ–Ω—É—Å –∑–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata_score = 0.1 if metadata.get("priority") == "high" else 0.0
        
        return min(1.0, content_score + keyword_score + access_score + metadata_score)
    
    @staticmethod
    def extract_keywords(text: str, max_keywords: int = 10) -> List[str]:
        """–ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è - –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ —Å—Ç–æ–∏—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å NLTK/spaCy
        words = text.lower().split()
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        stop_words = {"–∏", "–≤", "–Ω–∞", "—Å", "–ø–æ", "–¥–ª—è", "–æ—Ç", "–∫", "–æ", "—á—Ç–æ", "–∫–∞–∫"}
        keywords = [w for w in words if len(w) > 3 and w not in stop_words]
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–∞–º—ã–µ —á–∞—Å—Ç—ã–µ
        from collections import Counter
        return [word for word, _ in Counter(keywords).most_common(max_keywords)]


# === –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ===

@dataclass
class MemoryConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã –ø–∞–º—è—Ç–∏"""
    
    # –û–±—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    agent_id: str = "default_agent"
    storage_path: str = "memory_storage"
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–ª–æ—ë–≤
    short_term_capacity: int = 100
    long_term_capacity: int = 10000
    working_memory_ttl: int = 3600  # —Å–µ–∫—É–Ω–¥—ã
    
    # –í–µ–∫—Ç–æ—Ä–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2"
    vector_db_path: str = "vector_memory"
    similarity_threshold: float = 0.7
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
    auto_compress: bool = True
    compression_interval: int = 3600  # —Å–µ–∫—É–Ω–¥—ã
    cleanup_interval: int = 86400  # —Å–µ–∫—É–Ω–¥—ã
    
    # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
    batch_size: int = 32
    max_concurrent_operations: int = 10


class MemoryLogger:
    """–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Å–∏—Å—Ç–µ–º—ã –ø–∞–º—è—Ç–∏"""
    
    def __init__(self, agent_id: str):
        self.agent_id = agent_id
        self.logger = logging.getLogger(f"memory.{agent_id}")
    
    def store(self, entry_id: str, layer: str, content_length: int):
        self.logger.info(f"STORE: {entry_id} -> {layer} ({content_length} chars)")
    
    def search(self, query: str, results_count: int, layer: str):
        self.logger.info(f"SEARCH: '{query[:50]}...' -> {results_count} results in {layer}")
    
    def compress(self, before_count: int, after_count: int, layer: str):
        self.logger.info(f"COMPRESS: {before_count} -> {after_count} in {layer}")
    
    def optimize(self, layer: str, metrics: Dict[str, Any]):
        self.logger.info(f"OPTIMIZE: {layer} -> {metrics}")
    
    def error(self, operation: str, error: str):
        self.logger.error(f"ERROR: {operation} -> {error}")


# === –†–ê–ë–û–ß–ê–Ø –ü–ê–ú–Ø–¢–¨ (Working Memory Layer) ===

class WorkingMemoryLayer(BaseMemoryLayer):
    """
    –†–∞–±–æ—á–∞—è –ø–∞–º—è—Ç—å - –±—ã—Å—Ç—Ä–∞—è –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è —Ç–µ–∫—É—â–µ–π —Å–µ—Å—Å–∏–∏
    –•—Ä–∞–Ω–∏—Ç—Å—è –≤ RAM, –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—á–∏—â–∞–µ—Ç—Å—è –ø–æ TTL
    """
    
    def __init__(self, config: MemoryConfig):
        self.config = config
        self.entries: Dict[str, MemoryEntry] = {}
        self.access_times: Dict[str, datetime] = {}
        self.logger = MemoryLogger(config.agent_id)
    
    async def store(self, entry: MemoryEntry) -> bool:
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ —Ä–∞–±–æ—á—É—é –ø–∞–º—è—Ç—å"""
        try:
            # –í—ã—á–∏—Å–ª—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å
            entry.importance_score = MemoryUtils.calculate_importance(
                entry.content, entry.metadata, entry.access_count
            )
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º
            self.entries[entry.id] = entry
            self.access_times[entry.id] = datetime.now()
            
            # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            await self._cleanup_expired()
            
            self.logger.store(entry.id, "working", len(entry.content))
            return True
            
        except Exception as e:
            self.logger.error("store", str(e))
            return False
    
    async def search(self, query: str, limit: int = 5) -> List[MemorySearchResult]:
        """–ü–æ–∏—Å–∫ –≤ —Ä–∞–±–æ—á–µ–π –ø–∞–º—è—Ç–∏"""
        try:
            # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            query_words = set(query.lower().split())
            results = []
            
            for entry in self.entries.values():
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ –∏—Å—Ç–µ–∫–ª–∞ –ª–∏ –∑–∞–ø–∏—Å—å
                if self._is_expired(entry.id):
                    continue
                
                # –°—á–∏—Ç–∞–µ–º –ø–æ—Ö–æ–∂–µ—Å—Ç—å
                content_words = set(entry.content.lower().split())
                overlap = len(query_words.intersection(content_words))
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∞–∫–∂–µ —á–∞—Å—Ç–∏—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                if overlap == 0:
                    for query_word in query_words:
                        for content_word in content_words:
                            if query_word in content_word or content_word in query_word:
                                overlap += 0.5
                
                if overlap > 0:
                    similarity = overlap / max(len(query_words), len(content_words))
                    results.append(MemorySearchResult(
                        entry=entry,
                        similarity_score=similarity,
                        retrieval_reason=f"keyword_match_{overlap}"
                    ))
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫ –¥–æ—Å—Ç—É–ø–∞
                    entry.access_count += 1
                    self.access_times[entry.id] = datetime.now()
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            results.sort(key=lambda x: x.similarity_score, reverse=True)
            
            self.logger.search(query, len(results[:limit]), "working")
            return results[:limit]
            
        except Exception as e:
            self.logger.error("search", str(e))
            return []
    
    async def get(self, entry_id: str) -> Optional[MemoryEntry]:
        """–ü–æ–ª—É—á–∏—Ç—å –∑–∞–ø–∏—Å—å –ø–æ ID"""
        if entry_id in self.entries and not self._is_expired(entry_id):
            entry = self.entries[entry_id]
            entry.access_count += 1
            self.access_times[entry_id] = datetime.now()
            return entry
        return None
    
    async def delete(self, entry_id: str) -> bool:
        """–£–¥–∞–ª–∏—Ç—å –∑–∞–ø–∏—Å—å"""
        if entry_id in self.entries:
            del self.entries[entry_id]
            del self.access_times[entry_id]
            return True
        return False
    
    async def clear(self) -> bool:
        """–û—á–∏—Å—Ç–∏—Ç—å —Ä–∞–±–æ—á—É—é –ø–∞–º—è—Ç—å"""
        self.entries.clear()
        self.access_times.clear()
        return True
    
    async def get_stats(self) -> Dict[str, Any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞–±–æ—á–µ–π –ø–∞–º—è—Ç–∏"""
        active_entries = [e for e in self.entries.values() if not self._is_expired(e.id)]
        
        return {
            "layer": "working",
            "total_entries": len(self.entries),
            "active_entries": len(active_entries),
            "expired_entries": len(self.entries) - len(active_entries),
            "memory_usage_mb": sum(len(e.content) for e in self.entries.values()) / (1024 * 1024),
            "avg_importance": sum(e.importance_score for e in active_entries) / max(1, len(active_entries))
        }
    
    def _is_expired(self, entry_id: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏—Å—Ç–µ–∫–ª–∞ –ª–∏ –∑–∞–ø–∏—Å—å"""
        if entry_id not in self.access_times:
            return True
        
        age = datetime.now() - self.access_times[entry_id]
        return age.total_seconds() > self.config.working_memory_ttl
    
    async def _cleanup_expired(self):
        """–û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–µ–∫—à–∏–µ –∑–∞–ø–∏—Å–∏"""
        expired_ids = [
            entry_id for entry_id in self.entries.keys()
            if self._is_expired(entry_id)
        ]
        
        for entry_id in expired_ids:
            await self.delete(entry_id)


# === –ö–†–ê–¢–ö–û–°–†–û–ß–ù–ê–Ø –ü–ê–ú–Ø–¢–¨ (Short Term Memory Layer) ===

class ShortTermMemoryLayer(BaseMemoryLayer):
    """
    –ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–∞—è –ø–∞–º—è—Ç—å - –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π –±—É—Ñ–µ—Ä –≤–∞–∂–Ω—ã—Ö –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ SQLite, —Ä–∞–±–æ—Ç–∞–µ—Ç –ø–æ –ø—Ä–∏–Ω—Ü–∏–ø—É LRU
    """
    
    def __init__(self, config: MemoryConfig):
        self.config = config
        self.db_path = Path(config.storage_path) / f"{config.agent_id}_short_term.db"
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self.logger = MemoryLogger(config.agent_id)
        self._init_database()
    
    def _init_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SQLite –±–∞–∑—ã"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS short_term_memory (
                    id TEXT PRIMARY KEY,
                    content TEXT NOT NULL,
                    metadata TEXT NOT NULL,
                    timestamp TEXT NOT NULL,
                    access_count INTEGER DEFAULT 0,
                    importance_score REAL DEFAULT 0.0
                )
            """)
            conn.execute("CREATE INDEX IF NOT EXISTS idx_timestamp ON short_term_memory(timestamp)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_importance ON short_term_memory(importance_score)")
            conn.commit()
    
    async def store(self, entry: MemoryEntry) -> bool:
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—É—é –ø–∞–º—è—Ç—å"""
        try:
            # –í—ã—á–∏—Å–ª—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å
            entry.importance_score = MemoryUtils.calculate_importance(
                entry.content, entry.metadata, entry.access_count
            )
            
            with sqlite3.connect(self.db_path) as conn:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ –ø—Ä–µ–≤—ã—à–µ–Ω –ª–∏ –ª–∏–º–∏—Ç
                count = conn.execute("SELECT COUNT(*) FROM short_term_memory").fetchone()[0]
                
                if count >= self.config.short_term_capacity:
                    # –£–¥–∞–ª—è–µ–º –Ω–∞–∏–º–µ–Ω–µ–µ –≤–∞–∂–Ω—É—é –∑–∞–ø–∏—Å—å
                    conn.execute("""
                        DELETE FROM short_term_memory 
                        WHERE id = (
                            SELECT id FROM short_term_memory 
                            ORDER BY importance_score ASC, timestamp ASC 
                            LIMIT 1
                        )
                    """)
                
                # –í—Å—Ç–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å
                conn.execute("""
                    INSERT OR REPLACE INTO short_term_memory 
                    (id, content, metadata, timestamp, access_count, importance_score)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, (
                    entry.id,
                    entry.content,
                    json.dumps(entry.metadata),
                    entry.timestamp.isoformat(),
                    entry.access_count,
                    entry.importance_score
                ))
                conn.commit()
            
            self.logger.store(entry.id, "short_term", len(entry.content))
            return True
            
        except Exception as e:
            self.logger.error("store_short_term", str(e))
            return False
    
    async def search(self, query: str, limit: int = 5) -> List[MemorySearchResult]:
        """–ü–æ–∏—Å–∫ –≤ –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–π –ø–∞–º—è—Ç–∏"""
        try:
            query_words = set(query.lower().split())
            results = []
            
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute("""
                    SELECT id, content, metadata, timestamp, access_count, importance_score
                    FROM short_term_memory
                    ORDER BY importance_score DESC, timestamp DESC
                """)
                
                for row in cursor:
                    entry_id, content, metadata_json, timestamp_str, access_count, importance = row
                    
                    # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
                    content_words = set(content.lower().split())
                    overlap = len(query_words.intersection(content_words))
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∞–∫–∂–µ —á–∞—Å—Ç–∏—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                    if overlap == 0:
                        for query_word in query_words:
                            for content_word in content_words:
                                if query_word in content_word or content_word in query_word:
                                    overlap += 0.5
                    
                    if overlap > 0:
                        similarity = overlap / max(len(query_words), len(content_words))
                        
                        # –ë–æ–Ω—É—Å –∑–∞ –≤–∞–∂–Ω–æ—Å—Ç—å
                        similarity = similarity * (1 + importance * 0.3)
                        
                        entry = MemoryEntry(
                            id=entry_id,
                            content=content,
                            metadata=json.loads(metadata_json),
                            timestamp=datetime.fromisoformat(timestamp_str),
                            access_count=access_count,
                            importance_score=importance
                        )
                        
                        results.append(MemorySearchResult(
                            entry=entry,
                            similarity_score=min(1.0, similarity),
                            retrieval_reason=f"short_term_match_{overlap}"
                        ))
                        
                        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫ –¥–æ—Å—Ç—É–ø–∞
                        conn.execute(
                            "UPDATE short_term_memory SET access_count = access_count + 1 WHERE id = ?",
                            (entry_id,)
                        )
                
                conn.commit()
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            results.sort(key=lambda x: x.similarity_score, reverse=True)
            
            self.logger.search(query, len(results[:limit]), "short_term")
            return results[:limit]
            
        except Exception as e:
            self.logger.error("search_short_term", str(e))
            return []
    
    async def get(self, entry_id: str) -> Optional[MemoryEntry]:
        """–ü–æ–ª—É—á–∏—Ç—å –∑–∞–ø–∏—Å—å –ø–æ ID"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute("""
                    SELECT content, metadata, timestamp, access_count, importance_score
                    FROM short_term_memory WHERE id = ?
                """, (entry_id,))
                
                row = cursor.fetchone()
                if row:
                    content, metadata_json, timestamp_str, access_count, importance = row
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫ –¥–æ—Å—Ç—É–ø–∞
                    conn.execute(
                        "UPDATE short_term_memory SET access_count = access_count + 1 WHERE id = ?",
                        (entry_id,)
                    )
                    conn.commit()
                    
                    return MemoryEntry(
                        id=entry_id,
                        content=content,
                        metadata=json.loads(metadata_json),
                        timestamp=datetime.fromisoformat(timestamp_str),
                        access_count=access_count + 1,
                        importance_score=importance
                    )
        except Exception as e:
            self.logger.error("get_short_term", str(e))
        
        return None
    
    async def delete(self, entry_id: str) -> bool:
        """–£–¥–∞–ª–∏—Ç—å –∑–∞–ø–∏—Å—å"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute("DELETE FROM short_term_memory WHERE id = ?", (entry_id,))
                conn.commit()
                return cursor.rowcount > 0
        except Exception as e:
            self.logger.error("delete_short_term", str(e))
            return False
    
    async def clear(self) -> bool:
        """–û—á–∏—Å—Ç–∏—Ç—å –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—É—é –ø–∞–º—è—Ç—å"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.execute("DELETE FROM short_term_memory")
                conn.commit()
            return True
        except Exception as e:
            self.logger.error("clear_short_term", str(e))
            return False
    
    async def get_stats(self) -> Dict[str, Any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–π –ø–∞–º—è—Ç–∏"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute("""
                    SELECT 
                        COUNT(*) as total,
                        AVG(importance_score) as avg_importance,
                        MAX(importance_score) as max_importance,
                        SUM(LENGTH(content)) as total_content_size,
                        AVG(access_count) as avg_access_count
                    FROM short_term_memory
                """)
                
                row = cursor.fetchone()
                total, avg_importance, max_importance, total_size, avg_access = row
                
                return {
                    "layer": "short_term",
                    "total_entries": total or 0,
                    "capacity": self.config.short_term_capacity,
                    "utilization": (total or 0) / self.config.short_term_capacity,
                    "avg_importance": avg_importance or 0.0,
                    "max_importance": max_importance or 0.0,
                    "total_content_size": total_size or 0,
                    "avg_access_count": avg_access or 0.0
                }
        except Exception as e:
            self.logger.error("stats_short_term", str(e))
            return {"layer": "short_term", "error": str(e)}


# === –î–û–õ–ì–û–°–†–û–ß–ù–ê–Ø –ü–ê–ú–Ø–¢–¨ (Long Term Memory Layer) ===

class LongTermMemoryLayer(BaseMemoryLayer):
    """
    –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω–∞—è –ø–∞–º—è—Ç—å - –±–æ–ª—å—à–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º –ø–æ–∏—Å–∫–æ–º
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç SQLite + —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
    """
    
    def __init__(self, config: MemoryConfig):
        self.config = config
        self.db_path = Path(config.storage_path) / f"{config.agent_id}_long_term.db"
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self.logger = MemoryLogger(config.agent_id)
        self._embeddings_cache = {}  # –ö–µ—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self._init_database()
    
    def _init_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SQLite –±–∞–∑—ã –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –ø–∞–º—è—Ç–∏"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS long_term_memory (
                    id TEXT PRIMARY KEY,
                    content TEXT NOT NULL,
                    metadata TEXT NOT NULL,
                    timestamp TEXT NOT NULL,
                    access_count INTEGER DEFAULT 0,
                    importance_score REAL DEFAULT 0.0,
                    embedding_hash TEXT,
                    keywords TEXT,
                    compressed BOOLEAN DEFAULT 0
                )
            """)
            # –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
            conn.execute("CREATE INDEX IF NOT EXISTS idx_lt_timestamp ON long_term_memory(timestamp)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_lt_importance ON long_term_memory(importance_score)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_lt_keywords ON long_term_memory(keywords)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_lt_compressed ON long_term_memory(compressed)")
            conn.commit()
    
    def _simple_embedding(self, text: str) -> List[float]:
        """
        –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
        –í –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ —Å—Ç–æ–∏—Ç –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ sentence-transformers
        """
        # –ö–µ—à–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        if text in self._embeddings_cache:
            return self._embeddings_cache[text]
        
        # –ü—Ä–æ—Å—Ç–∞—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤
        words = text.lower().split()
        word_counts = {}
        for word in words:
            word_counts[word] = word_counts.get(word, 0) + 1
        
        # –°–æ–∑–¥–∞—ë–º –≤–µ–∫—Ç–æ—Ä –∏–∑ 128 –∏–∑–º–µ—Ä–µ–Ω–∏–π
        vector = [0.0] * 128
        for i, word in enumerate(word_counts.keys()):
            if i >= 128:
                break
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º hash –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ–≥–æ –º–∞–ø–ø–∏–Ω–≥–∞ —Å–ª–æ–≤ –≤ –∏–∑–º–µ—Ä–µ–Ω–∏—è
            dim = hash(word) % 128
            vector[dim] = word_counts[word] / len(words)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ–∫—Ç–æ—Ä
        norm = sum(x*x for x in vector) ** 0.5
        if norm > 0:
            vector = [x/norm for x in vector]
        
        self._embeddings_cache[text] = vector
        return vector
    
    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏"""
        if len(vec1) != len(vec2):
            return 0.0
        
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        norm1 = sum(a * a for a in vec1) ** 0.5
        norm2 = sum(b * b for b in vec2) ** 0.5
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)
    
    async def store(self, entry: MemoryEntry) -> bool:
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—É—é –ø–∞–º—è—Ç—å"""
        try:
            # –í—ã—á–∏—Å–ª—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å
            entry.importance_score = MemoryUtils.calculate_importance(
                entry.content, entry.metadata, entry.access_count
            )
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
            embedding = self._simple_embedding(entry.content)
            embedding_hash = hashlib.md5(str(embedding).encode()).hexdigest()[:16]
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            keywords = MemoryUtils.extract_keywords(entry.content)
            keywords_str = " ".join(keywords)
            
            with sqlite3.connect(self.db_path) as conn:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ –ø—Ä–µ–≤—ã—à–µ–Ω –ª–∏ –ª–∏–º–∏—Ç
                count = conn.execute("SELECT COUNT(*) FROM long_term_memory").fetchone()[0]
                
                if count >= self.config.long_term_capacity:
                    # –£–¥–∞–ª—è–µ–º –Ω–∞–∏–º–µ–Ω–µ–µ –≤–∞–∂–Ω—É—é –∑–∞–ø–∏—Å—å
                    conn.execute("""
                        DELETE FROM long_term_memory 
                        WHERE id = (
                            SELECT id FROM long_term_memory 
                            WHERE compressed = 0
                            ORDER BY importance_score ASC, access_count ASC, timestamp ASC 
                            LIMIT 1
                        )
                    """)
                
                # –í—Å—Ç–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å
                conn.execute("""
                    INSERT OR REPLACE INTO long_term_memory 
                    (id, content, metadata, timestamp, access_count, importance_score, 
                     embedding_hash, keywords, compressed)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    entry.id,
                    entry.content,
                    json.dumps(entry.metadata),
                    entry.timestamp.isoformat(),
                    entry.access_count,
                    entry.importance_score,
                    embedding_hash,
                    keywords_str,
                    0  # not compressed
                ))
                conn.commit()
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –≤ –∫–µ—à
                entry.embedding = embedding
            
            self.logger.store(entry.id, "long_term", len(entry.content))
            return True
            
        except Exception as e:
            self.logger.error("store_long_term", str(e))
            return False
    
    async def search(self, query: str, limit: int = 5) -> List[MemorySearchResult]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –≤ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –ø–∞–º—è—Ç–∏"""
        try:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = self._simple_embedding(query)
            query_words = set(query.lower().split())
            results = []
            
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute("""
                    SELECT id, content, metadata, timestamp, access_count, 
                           importance_score, keywords
                    FROM long_term_memory
                    ORDER BY importance_score DESC, timestamp DESC
                """)
                
                for row in cursor:
                    entry_id, content, metadata_json, timestamp_str, access_count, importance, keywords = row
                    
                    # –í—ã—á–∏—Å–ª—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
                    content_embedding = self._simple_embedding(content)
                    semantic_similarity = self._cosine_similarity(query_embedding, content_embedding)
                    
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
                    keyword_overlap = 0
                    if keywords:
                        keyword_words = set(keywords.split())
                        keyword_overlap = len(query_words.intersection(keyword_words))
                    
                    # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                    combined_score = (
                        semantic_similarity * 0.7 +  # 70% —Å–µ–º–∞–Ω—Ç–∏–∫–∞
                        (keyword_overlap / max(1, len(query_words))) * 0.2 +  # 20% –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
                        importance * 0.1  # 10% –≤–∞–∂–Ω–æ—Å—Ç—å
                    )
                    
                    if combined_score > 0.05:  # –ü–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (–ø–æ–Ω–∏–∂–µ–Ω –¥–ª—è —Ç–µ—Å—Ç–æ–≤)
                        entry = MemoryEntry(
                            id=entry_id,
                            content=content,
                            metadata=json.loads(metadata_json),
                            timestamp=datetime.fromisoformat(timestamp_str),
                            access_count=access_count,
                            importance_score=importance,
                            embedding=content_embedding
                        )
                        
                        results.append(MemorySearchResult(
                            entry=entry,
                            similarity_score=combined_score,
                            retrieval_reason=f"semantic_{semantic_similarity:.2f}_keywords_{keyword_overlap}"
                        ))
                        
                        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫ –¥–æ—Å—Ç—É–ø–∞
                        conn.execute(
                            "UPDATE long_term_memory SET access_count = access_count + 1 WHERE id = ?",
                            (entry_id,)
                        )
                
                conn.commit()
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            results.sort(key=lambda x: x.similarity_score, reverse=True)
            
            self.logger.search(query, len(results[:limit]), "long_term")
            return results[:limit]
            
        except Exception as e:
            self.logger.error("search_long_term", str(e))
            return []
    
    async def get(self, entry_id: str) -> Optional[MemoryEntry]:
        """–ü–æ–ª—É—á–∏—Ç—å –∑–∞–ø–∏—Å—å –ø–æ ID"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute("""
                    SELECT content, metadata, timestamp, access_count, importance_score, keywords
                    FROM long_term_memory WHERE id = ?
                """, (entry_id,))
                
                row = cursor.fetchone()
                if row:
                    content, metadata_json, timestamp_str, access_count, importance, keywords = row
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫ –¥–æ—Å—Ç—É–ø–∞
                    conn.execute(
                        "UPDATE long_term_memory SET access_count = access_count + 1 WHERE id = ?",
                        (entry_id,)
                    )
                    conn.commit()
                    
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø–∏—Å–∏
                    embedding = self._simple_embedding(content)
                    
                    return MemoryEntry(
                        id=entry_id,
                        content=content,
                        metadata=json.loads(metadata_json),
                        timestamp=datetime.fromisoformat(timestamp_str),
                        access_count=access_count + 1,
                        importance_score=importance,
                        embedding=embedding
                    )
        except Exception as e:
            self.logger.error("get_long_term", str(e))
        
        return None
    
    async def delete(self, entry_id: str) -> bool:
        """–£–¥–∞–ª–∏—Ç—å –∑–∞–ø–∏—Å—å"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute("DELETE FROM long_term_memory WHERE id = ?", (entry_id,))
                conn.commit()
                return cursor.rowcount > 0
        except Exception as e:
            self.logger.error("delete_long_term", str(e))
            return False
    
    async def clear(self) -> bool:
        """–û—á–∏—Å—Ç–∏—Ç—å –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—É—é –ø–∞–º—è—Ç—å"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.execute("DELETE FROM long_term_memory")
                conn.commit()
            # –û—á–∏—â–∞–µ–º –∫–µ—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            self._embeddings_cache.clear()
            return True
        except Exception as e:
            self.logger.error("clear_long_term", str(e))
            return False
    
    async def get_stats(self) -> Dict[str, Any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –ø–∞–º—è—Ç–∏"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute("""
                    SELECT 
                        COUNT(*) as total,
                        COUNT(CASE WHEN compressed = 1 THEN 1 END) as compressed_count,
                        AVG(importance_score) as avg_importance,
                        MAX(importance_score) as max_importance,
                        SUM(LENGTH(content)) as total_content_size,
                        AVG(access_count) as avg_access_count,
                        MAX(access_count) as max_access_count
                    FROM long_term_memory
                """)
                
                row = cursor.fetchone()
                total, compressed, avg_importance, max_importance, total_size, avg_access, max_access = row
                
                return {
                    "layer": "long_term",
                    "total_entries": total or 0,
                    "compressed_entries": compressed or 0,
                    "uncompressed_entries": (total or 0) - (compressed or 0),
                    "capacity": self.config.long_term_capacity,
                    "utilization": (total or 0) / self.config.long_term_capacity,
                    "avg_importance": avg_importance or 0.0,
                    "max_importance": max_importance or 0.0,
                    "total_content_size": total_size or 0,
                    "avg_access_count": avg_access or 0.0,
                    "max_access_count": max_access or 0,
                    "embeddings_cached": len(self._embeddings_cache)
                }
        except Exception as e:
            self.logger.error("stats_long_term", str(e))
            return {"layer": "long_term", "error": str(e)} 