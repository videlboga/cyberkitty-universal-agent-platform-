#!/usr/bin/env python3
"""
üï∑Ô∏è –û–¢–õ–ê–î–ö–ê: enhanced_web_scraping_tool
–ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ async –≤—ã–∑–æ–≤—ã –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã

–ü–†–û–ë–õ–ï–ú–´ –ò–ó –ü–ê–ú–Ø–¢–ò:
- –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç —Ç—Ä–µ–±—É–µ—Ç async/await
- –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–Ω–µ—Ç action, –µ—Å—Ç—å urls)
- –í–æ–∑–≤—Ä–∞—â–∞–ª –ø—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ

–ü–õ–ê–ù –û–¢–õ–ê–î–ö–ò:
1. –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π async –≤—ã–∑–æ–≤
2. –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (urls –≤–º–µ—Å—Ç–æ url)
3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å–∫—Ä–∞–ø–∏–Ω–≥-–¥–∞–Ω–Ω—ã—Ö
"""

import asyncio
import time
import json

# –ò–º–ø–æ—Ä—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
try:
    from kittycore.tools.enhanced_web_scraping_tool import EnhancedWebScrapingTool
    IMPORT_OK = True
    print("‚úÖ –ò–º–ø–æ—Ä—Ç enhanced_web_scraping_tool —É—Å–ø–µ—à–µ–Ω")
except ImportError as e:
    print(f"‚ùå –ò–ú–ü–û–†–¢ –û–®–ò–ë–ö–ê: {e}")
    IMPORT_OK = False

async def test_web_scraping_correct():
    """–ü—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ç–µ—Å—Ç —Å async –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
    if not IMPORT_OK:
        return "IMPORT_ERROR"
    
    print("\nüï∑Ô∏è –¢–µ—Å—Ç–∏—Ä—É—é enhanced_web_scraping —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏...")
    start_time = time.time()
    
    tool = EnhancedWebScrapingTool()
    
    # –ü–†–ê–í–ò–õ–¨–ù–´–ô –≤—ã–∑–æ–≤: await + urls (–ù–ï action, –ù–ï url)
    result = await tool.execute(
        urls=["https://httpbin.org/html"],
        extract_links=True,
        extract_metadata=True
    )
    
    execution_time = time.time() - start_time
    
    print(f"‚è±Ô∏è –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {execution_time:.2f}—Å")
    print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç: {type(result)}")
    
    if isinstance(result, dict):
        print(f"‚úÖ Success: {result.get('success', '–ù–ï–¢')}")
        if result.get('success'):
            print(f"üì¶ –í—Å–µ–≥–æ URL: {result.get('total_urls', 0)}")
            print(f"üéØ –£—Å–ø–µ—à–Ω—ã—Ö —Å–∫—Ä–∞–ø–∏–Ω–≥–æ–≤: {result.get('successful_scrapes', 0)}")
            
            # –ü—Ä–æ–≤–µ—Ä–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            results = result.get('results', [])
            if results:
                first_result = results[0]
                print(f"üîç –ü–µ—Ä–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:")
                print(f"   URL: {first_result.get('url', '–ù–ï–¢')}")
                print(f"   Success: {first_result.get('success', '–ù–ï–¢')}")
                print(f"   –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {first_result.get('content_length', 0)} –±–∞–π—Ç")
                if first_result.get('success'):
                    if 'text_preview' in first_result:
                        preview = first_result['text_preview'][:100]
                        print(f"   –¢–µ–∫—Å—Ç: {preview}...")
                    if 'metadata' in first_result:
                        metadata = first_result['metadata']
                        print(f"   –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {len(metadata)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
                        if 'title' in metadata:
                            print(f"   –ó–∞–≥–æ–ª–æ–≤–æ–∫: {metadata['title'][:50]}...")
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞: {result.get('error', '–ù–ï–ò–ó–í–ï–°–¢–ù–û')}")
    
    return result

async def test_web_scraping_multiple():
    """–¢–µ—Å—Ç —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ URL"""
    if not IMPORT_OK:
        return "IMPORT_ERROR"
    
    print("\nüï∑Ô∏è –¢–µ—Å—Ç enhanced_web_scraping —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ URL...")
    
    tool = EnhancedWebScrapingTool()
    
    # –¢–µ—Å—Ç —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –Ω–∞–¥–µ–∂–Ω—ã–º–∏ URL
    result = await tool.execute(
        urls=[
            "https://httpbin.org/html",
            "https://httpbin.org/robots.txt"
        ],
        extract_links=True,
        extract_images=False,
        max_content_length=10000
    )
    
    print(f"üìä –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ç–µ—Å—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {type(result)}")
    if isinstance(result, dict):
        print(f"‚úÖ Success: {result.get('success')}")
        print(f"üì¶ –í—Å–µ–≥–æ URL: {result.get('total_urls', 0)}")
        print(f"üéØ –£—Å–ø–µ—à–Ω—ã—Ö: {result.get('successful_scrapes', 0)}")
    
    return result

async def test_web_scraping_minimal():
    """–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç —Å –æ–¥–Ω–∏–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º"""
    if not IMPORT_OK:
        return "IMPORT_ERROR"
    
    print("\nüï∑Ô∏è –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç enhanced_web_scraping...")
    
    tool = EnhancedWebScrapingTool()
    
    # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≤—ã–∑–æ–≤ - —Ç–æ–ª—å–∫–æ urls
    result = await tool.execute(urls=["https://httpbin.org/html"])
    
    print(f"üìä –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {type(result)}")
    if isinstance(result, dict):
        print(f"‚úÖ Success: {result.get('success')}")
        if result.get('success'):
            total_size = 0
            for res in result.get('results', []):
                if res.get('success'):
                    total_size += res.get('content_length', 0)
            print(f"üì¶ –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {total_size} –±–∞–π—Ç")
    
    return result

def is_result_honest(result, test_name):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Å—Ç–Ω–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"""
    if not result:
        print(f"‚ùå {test_name}: –ü—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
        return False
    
    if not isinstance(result, dict):
        print(f"‚ùå {test_name}: –†–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ dict")
        return False
    
    if not result.get('success'):
        print(f"‚ùå {test_name}: success=False")
        if 'error' in result:
            print(f"   –û—à–∏–±–∫–∞: {result['error']}")
        return False
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö
    required_keys = ['total_urls', 'successful_scrapes', 'results']
    for key in required_keys:
        if key not in result:
            print(f"‚ùå {test_name}: –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–ª—é—á {key}")
            return False
    
    results = result.get('results', [])
    if not results:
        print(f"‚ùå {test_name}: –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞")
        return False
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    first_result = results[0]
    if not first_result.get('success'):
        print(f"‚ùå {test_name}: –ü–µ—Ä–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ—É—Å–ø–µ—à–µ–Ω")
        if 'error' in first_result:
            print(f"   –û—à–∏–±–∫–∞: {first_result['error']}")
        return False
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ñ–µ–π–∫–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    result_str = str(result)
    fake_patterns = [
        "enhanced_web_scraping: —É—Å–ø–µ—à–Ω–æ",
        "–¥–µ–º–æ —Å–∫—Ä–∞–ø–∏–Ω–≥",
        "–∑–∞–≥–ª—É—à–∫–∞ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞",
        "—Ç–µ—Å—Ç–æ–≤—ã–µ HTML –¥–∞–Ω–Ω—ã–µ"
    ]
    
    for pattern in fake_patterns:
        if pattern.lower() in result_str.lower():
            print(f"‚ùå {test_name}: –ù–∞–π–¥–µ–Ω —Ñ–µ–π–∫–æ–≤—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω: {pattern}")
            return False
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ —Å–∫—Ä–∞–ø–∏–Ω–≥-–ø—Ä–∏–∑–Ω–∞–∫–∏
    scraping_indicators = [
        "content_length", "text_preview", "metadata", "httpbin", "response_time"
    ]
    
    has_scraping_data = any(indicator in result_str.lower() for indicator in scraping_indicators)
    
    if not has_scraping_data:
        print(f"‚ùå {test_name}: –ù–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Ä–µ–∞–ª—å–Ω–æ–≥–æ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞")
        return False
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
    total_content = sum(
        res.get('content_length', 0) 
        for res in results 
        if res.get('success')
    )
    
    if total_content < 100:
        print(f"‚ùå {test_name}: –°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç ({total_content} –±–∞–π—Ç)")
        return False
    
    print(f"‚úÖ {test_name}: –ß–ï–°–¢–ù–´–ô —Ä–µ–∑—É–ª—å—Ç–∞—Ç ({len(result_str)} –±–∞–π—Ç, –∫–æ–Ω—Ç–µ–Ω—Ç {total_content} –±–∞–π—Ç)")
    return True

async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ—Ç–ª–∞–¥–∫–∏"""
    print("üï∑Ô∏è –û–¢–õ–ê–î–ö–ê: enhanced_web_scraping_tool")
    print("=" * 50)
    
    if not IMPORT_OK:
        print("‚ùå –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å - –æ—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞")
        return
    
    results = {}
    
    # –¢–µ—Å—Ç 1: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ–ª–Ω—ã–π —Ç–µ—Å—Ç
    print("\n" + "=" * 30)
    print("–¢–ï–°–¢ 1: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã")
    try:
        result1 = await test_web_scraping_correct()
        results["correct_params"] = is_result_honest(result1, "–ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã")
    except Exception as e:
        print(f"‚ùå –¢–ï–°–¢ 1 –û–®–ò–ë–ö–ê: {e}")
        results["correct_params"] = False
    
    # –¢–µ—Å—Ç 2: –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ URL
    print("\n" + "=" * 30)
    print("–¢–ï–°–¢ 2: –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ URL")
    try:
        result2 = await test_web_scraping_multiple()
        results["multiple_urls"] = is_result_honest(result2, "–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ URL")
    except Exception as e:
        print(f"‚ùå –¢–ï–°–¢ 2 –û–®–ò–ë–ö–ê: {e}")
        results["multiple_urls"] = False
    
    # –¢–µ—Å—Ç 3: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    print("\n" + "=" * 30)
    print("–¢–ï–°–¢ 3: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã")
    try:
        result3 = await test_web_scraping_minimal()
        results["minimal_params"] = is_result_honest(result3, "–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã")
    except Exception as e:
        print(f"‚ùå –¢–ï–°–¢ 3 –û–®–ò–ë–ö–ê: {e}")
        results["minimal_params"] = False
    
    # –ò—Ç–æ–≥–∏
    print("\n" + "=" * 50)
    print("üìä –ò–¢–û–ì–ò –û–¢–õ–ê–î–ö–ò:")
    
    total_tests = len(results)
    passed_tests = sum(1 for success in results.values() if success)
    success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0
    
    print(f"–í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤: {total_tests}")
    print(f"–ü—Ä–æ—à–ª–æ —Ç–µ—Å—Ç–æ–≤: {passed_tests}")
    print(f"–ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {success_rate:.1f}%")
    
    print("\n–î–µ—Ç–∞–ª–∏:")
    for test_name, success in results.items():
        status = "‚úÖ –ü–†–û–®–ï–õ" if success else "‚ùå –ü–†–û–í–ê–õ–ï–ù"
        print(f"  {test_name}: {status}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    with open("web_scraping_fix_results.json", "w", encoding="utf-8") as f:
        json.dump({
            "tool": "enhanced_web_scraping_tool",
            "tests": results,
            "summary": {
                "total_tests": total_tests,
                "passed_tests": passed_tests,
                "success_rate": success_rate
            },
            "timestamp": time.strftime('%Y-%m-%d %H:%M:%S')
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ web_scraping_fix_results.json")
    
    # –í–µ—Ä–¥–∏–∫—Ç
    if success_rate >= 66:
        print("\nüéâ ENHANCED_WEB_SCRAPING_TOOL –ò–°–ü–†–ê–í–õ–ï–ù!")
        print("–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ async –≤—ã–∑–æ–≤–∞–º–∏ –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏")
        return True
    elif success_rate >= 33:
        print("\n‚ö†Ô∏è ENHANCED_WEB_SCRAPING_TOOL –ß–ê–°–¢–ò–ß–ù–û –†–ê–ë–û–¢–ê–ï–¢")
        print("–¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ç–ª–∞–¥–∫–∞")
        return False
    else:
        print("\n‚ùå ENHANCED_WEB_SCRAPING_TOOL –ù–ï –†–ê–ë–û–¢–ê–ï–¢")
        print("–¢—Ä–µ–±—É–µ—Ç—Å—è —Å–µ—Ä—å–µ–∑–Ω–∞—è –¥–æ—Ä–∞–±–æ—Ç–∫–∞")
        return False

if __name__ == "__main__":
    asyncio.run(main()) 