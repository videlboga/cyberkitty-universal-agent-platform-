"""
LLM Provider - –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ LLM

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
- OpenRouter (100+ –º–æ–¥–µ–ª–µ–π, –≤–∫–ª—é—á–∞—è –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ)
- Anthropic Claude
- Local –º–æ–¥–µ–ª–∏ (Ollama)
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
"""

import logging
import json
import os
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional, Iterator
from dataclasses import dataclass
from datetime import datetime

try:
    from .config import Config, get_config
except ImportError:
    # Fallback –¥–ª—è —Å–ª—É—á–∞–µ–≤ –∫–æ–≥–¥–∞ –º–æ–¥—É–ª—å –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ
    import sys
    import os
    sys.path.insert(0, os.path.dirname(__file__))
    from config import Config, get_config

logger = logging.getLogger(__name__)


@dataclass
class LLMMessage:
    """–°–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è LLM"""
    role: str  # "system", "user", "assistant"
    content: str
    timestamp: Optional[str] = None


@dataclass 
class LLMResponse:
    """–û—Ç–≤–µ—Ç –æ—Ç LLM"""
    content: str
    model: str
    usage: Optional[Dict[str, int]] = None
    metadata: Optional[Dict[str, Any]] = None


class LLMProvider(ABC):
    """–ë–∞–∑–æ–≤—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä LLM"""
    
    def __init__(self, model: str, config: Config):
        self.model = model
        self.config = config
        self.request_count = 0
        self.total_tokens = 0
    
    @abstractmethod
    def complete(self, prompt: str, **kwargs) -> str:
        """–ü—Ä–æ—Å—Ç–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞"""
        pass
    
    @abstractmethod
    def chat(self, messages: List[LLMMessage], **kwargs) -> LLMResponse:
        """–ß–∞—Ç —Å –∏—Å—Ç–æ—Ä–∏–µ–π —Å–æ–æ–±—â–µ–Ω–∏–π"""
        pass
    
    @abstractmethod
    def stream(self, prompt: str, **kwargs) -> Iterator[str]:
        """–°—Ç—Ä–∏–º–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞"""
        pass
    
    def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
        return {
            "model": self.model,
            "request_count": self.request_count,
            "total_tokens": self.total_tokens,
            "provider": self.__class__.__name__
        }


class OpenAIProvider(LLMProvider):
    """–ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è OpenRouter API (—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Å OpenAI)"""
    
    def __init__(self, model: str = "deepseek/deepseek-chat", config: Optional[Config] = None):
        config = config or get_config()
        super().__init__(model, config)
        
        if not config.openai_api_key:
            raise ValueError("OpenRouter API key –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
        
        self.api_key = config.openai_api_key
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º OpenRouter –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é, —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å OpenAI
        self.base_url = getattr(config, 'openai_base_url', None) or os.getenv('OPENAI_BASE_URL', 'https://openrouter.ai/api/v1')
    
    def _parse_prompt_to_messages(self, prompt: str) -> List[Dict[str, str]]:
        """–ü–∞—Ä—Å–∏—Ç –ø—Ä–æ–º–ø—Ç –≤ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Å–æ–æ–±—â–µ–Ω–∏–π"""
        messages = []
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø—Ä–æ–º–ø—Ç –Ω–∞ —Å—Ç—Ä–æ–∫–∏
        lines = prompt.strip().split('\n')
        current_role = None
        current_content = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # –ò—â–µ–º —Ä–æ–ª–∏
            if line.startswith('System: '):
                if current_role and current_content:
                    messages.append({'role': current_role, 'content': '\n'.join(current_content)})
                current_role = 'system'
                current_content = [line[8:]]  # –£–±–∏—Ä–∞–µ–º "System: "
            elif line.startswith('User: '):
                if current_role and current_content:
                    messages.append({'role': current_role, 'content': '\n'.join(current_content)})
                current_role = 'user'
                current_content = [line[6:]]  # –£–±–∏—Ä–∞–µ–º "User: "
            elif line.startswith('Assistant:') and line != 'Assistant:':
                if current_role and current_content:
                    messages.append({'role': current_role, 'content': '\n'.join(current_content)})
                current_role = 'assistant'
                current_content = [line[10:]]  # –£–±–∏—Ä–∞–µ–º "Assistant:"
            elif line == 'Assistant:':
                # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø—É—Å—Ç–æ–π "Assistant:" –≤ –∫–æ–Ω—Ü–µ
                break
            else:
                # –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
                if current_content:
                    current_content.append(line)
                else:
                    # –ï—Å–ª–∏ –Ω–µ—Ç —Ä–æ–ª–∏, –¥–æ–±–∞–≤–ª—è–µ–º –∫–∞–∫ user
                    if not current_role:
                        current_role = 'user'
                    current_content.append(line)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        if current_role and current_content:
            messages.append({'role': current_role, 'content': '\n'.join(current_content)})
        
        # –ï—Å–ª–∏ –Ω–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–π, —Å–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤–æ–µ
        if not messages:
            messages = [{'role': 'user', 'content': prompt}]
        
        return messages
    
    def complete(self, prompt: str, **kwargs) -> str:
        """–ü—Ä–æ—Å—Ç–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —á–µ—Ä–µ–∑ OpenRouter"""
        try:
            self.request_count += 1
            
            # –ü—Ä–æ–±—É–µ–º –Ω–∞—Å—Ç–æ—è—â–∏–π API –µ—Å–ª–∏ –µ—Å—Ç—å –∫–ª—é—á
            if self.api_key and self.api_key != 'test-key':
                try:
                    import requests
                    
                    headers = {
                        'Authorization': f'Bearer {self.api_key}',
                        'Content-Type': 'application/json',
                    }
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è OpenRouter
                    if 'openrouter.ai' in self.base_url:
                        headers.update({
                            'HTTP-Referer': 'https://github.com/kittycore/kittycore',
                            'X-Title': 'KittyCore Agent Platform'
                        })
                    
                    # –ü–∞—Ä—Å–∏–º –ø—Ä–æ–º–ø—Ç –≤ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Å–æ–æ–±—â–µ–Ω–∏–π
                    messages = self._parse_prompt_to_messages(prompt)
                    
                    data = {
                        'model': self.model,
                        'messages': messages,
                        'max_tokens': kwargs.get('max_tokens', 1000),
                        'temperature': kwargs.get('temperature', 0.7)
                    }
                    
                    response = requests.post(
                        f"{self.base_url}/chat/completions",
                        headers=headers,
                        json=data,
                        timeout=30
                    )
                    
                    if response.status_code == 200:
                        result = response.json()
                        content = result['choices'][0]['message']['content']
                        logger.info(f"OpenRouter API –∑–∞–ø—Ä–æ—Å –≤—ã–ø–æ–ª–Ω–µ–Ω, –º–æ–¥–µ–ª—å: {self.model}")
                        return content
                    else:
                        logger.warning(f"API –æ—à–∏–±–∫–∞ {response.status_code}, –∏—Å–ø–æ–ª—å–∑—É–µ–º mock")
                        logger.warning(f"–ó–∞–ø—Ä–æ—Å: {json.dumps(data, indent=2)}")
                        logger.warning(f"–û—Ç–≤–µ—Ç: {response.text}")
                        
                except Exception as api_error:
                    logger.warning(f"API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {api_error}, –∏—Å–ø–æ–ª—å–∑—É–µ–º mock")
            
            # Fallback: —Å–∏–º—É–ª—è—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
            response = f"ü§ñ KittyCore Mock Response –¥–ª—è '{prompt[:30]}...' –æ—Ç –º–æ–¥–µ–ª–∏ {self.model}"
            logger.info(f"Mock –æ—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –¥–ª—è –º–æ–¥–µ–ª–∏: {self.model}")
            return response
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ OpenRouter API: {e}")
            return f"–û—à–∏–±–∫–∞: {str(e)}"
    
    def chat(self, messages: List[LLMMessage], **kwargs) -> LLMResponse:
        """–ß–∞—Ç —á–µ—Ä–µ–∑ OpenRouter API"""
        try:
            self.request_count += 1
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç OpenRouter (—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Å OpenAI)
            openai_messages = [
                {"role": msg.role, "content": msg.content}
                for msg in messages
            ]
            
            # –°–∏–º—É–ª—è—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
            last_message = messages[-1].content if messages else ""
            content = f"–ß–∞—Ç –æ—Ç–≤–µ—Ç OpenRouter {self.model}: {last_message[:30]}..."
            
            return LLMResponse(
                content=content,
                model=self.model,
                usage={"prompt_tokens": 100, "completion_tokens": 50},
                metadata={"provider": "openrouter"}
            )
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ OpenRouter chat: {e}")
            raise
    
    def stream(self, prompt: str, **kwargs) -> Iterator[str]:
        """–°—Ç—Ä–∏–º–∏–Ω–≥ –æ—Ç OpenRouter"""
        try:
            self.request_count += 1
            
            # –°–∏–º—É–ª—è—Ü–∏—è —Å—Ç—Ä–∏–º–∏–Ω–≥–∞
            response = f"–°—Ç—Ä–∏–º –æ—Ç–≤–µ—Ç –æ—Ç OpenRouter {self.model} –Ω–∞: {prompt[:30]}..."
            
            for word in response.split():
                yield word + " "
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ OpenRouter streaming: {e}")
            raise


class AnthropicProvider(LLMProvider):
    """–ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è Anthropic Claude"""
    
    def __init__(self, model: str = "claude-3-haiku-20240307", config: Optional[Config] = None):
        config = config or get_config()
        super().__init__(model, config)
        
        if not config.anthropic_api_key:
            raise ValueError("Anthropic API key –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
        
        self.api_key = config.anthropic_api_key
        self.base_url = "https://api.anthropic.com/v1"
    
    def complete(self, prompt: str, **kwargs) -> str:
        """–ü—Ä–æ—Å—Ç–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —á–µ—Ä–µ–∑ Anthropic"""
        try:
            self.request_count += 1
            
            # –°–∏–º—É–ª—è—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
            response = f"–û—Ç–≤–µ—Ç Claude {self.model} –Ω–∞ –ø—Ä–æ–º–ø—Ç: {prompt[:50]}..."
            
            logger.info(f"Anthropic –∑–∞–ø—Ä–æ—Å –≤—ã–ø–æ–ª–Ω–µ–Ω, –º–æ–¥–µ–ª—å: {self.model}")
            return response
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ Anthropic API: {e}")
            raise
    
    def chat(self, messages: List[LLMMessage], **kwargs) -> LLMResponse:
        """–ß–∞—Ç —á–µ—Ä–µ–∑ Anthropic API"""
        try:
            self.request_count += 1
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç Anthropic
            last_message = messages[-1].content if messages else ""
            content = f"Claude –æ—Ç–≤–µ—Ç {self.model}: {last_message[:30]}..."
            
            return LLMResponse(
                content=content,
                model=self.model,
                usage={"input_tokens": 100, "output_tokens": 50},
                metadata={"provider": "anthropic"}
            )
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ Anthropic chat: {e}")
            raise
    
    def stream(self, prompt: str, **kwargs) -> Iterator[str]:
        """–°—Ç—Ä–∏–º–∏–Ω–≥ –æ—Ç Anthropic"""
        try:
            self.request_count += 1
            
            # –°–∏–º—É–ª—è—Ü–∏—è —Å—Ç—Ä–∏–º–∏–Ω–≥–∞
            response = f"–°—Ç—Ä–∏–º –æ—Ç–≤–µ—Ç –æ—Ç Claude {self.model} –Ω–∞: {prompt[:30]}..."
            
            for word in response.split():
                yield word + " "
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ Anthropic streaming: {e}")
            raise


class LocalLLMProvider(LLMProvider):
    """–ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (—á–µ—Ä–µ–∑ Ollama –∏ –¥—Ä.)"""
    
    def __init__(self, model: str = "llama3", config: Optional[Config] = None):
        config = config or get_config()
        super().__init__(model, config)
        
        self.base_url = "http://localhost:11434"  # Ollama default
    
    def complete(self, prompt: str, **kwargs) -> str:
        """–ü—Ä–æ—Å—Ç–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å"""
        try:
            self.request_count += 1
            
            # –°–∏–º—É–ª—è—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
            response = f"–õ–æ–∫–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç {self.model} –Ω–∞ –ø—Ä–æ–º–ø—Ç: {prompt[:50]}..."
            
            logger.info(f"–õ–æ–∫–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å –≤—ã–ø–æ–ª–Ω–µ–Ω, –º–æ–¥–µ–ª—å: {self.model}")
            return response
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}")
            raise
    
    def chat(self, messages: List[LLMMessage], **kwargs) -> LLMResponse:
        """–ß–∞—Ç —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å"""
        try:
            self.request_count += 1
            
            last_message = messages[-1].content if messages else ""
            content = f"–õ–æ–∫–∞–ª—å–Ω—ã–π —á–∞—Ç {self.model}: {last_message[:30]}..."
            
            return LLMResponse(
                content=content,
                model=self.model,
                usage={"tokens": 150},
                metadata={"provider": "local"}
            )
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —á–∞—Ç–∞: {e}")
            raise
    
    def stream(self, prompt: str, **kwargs) -> Iterator[str]:
        """–°—Ç—Ä–∏–º–∏–Ω–≥ –æ—Ç –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            self.request_count += 1
            
            response = f"–õ–æ–∫–∞–ª—å–Ω—ã–π —Å—Ç—Ä–∏–º {self.model} –Ω–∞: {prompt[:30]}..."
            
            for word in response.split():
                yield word + " "
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Å—Ç—Ä–∏–º–∏–Ω–≥–∞: {e}")
            raise


# –ö–∞—Ä—Ç–∞ –º–æ–¥–µ–ª–µ–π –∫ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º (–≤—Å–µ —á–µ—Ä–µ–∑ OpenRouter)
MODEL_PROVIDERS = {
    # === –ë–ï–°–ü–õ–ê–¢–ù–´–ï –ú–û–î–ï–õ–ò (–¥–ª—è —Ç–µ—Å—Ç–æ–≤) ===
    "deepseek/deepseek-chat": OpenAIProvider,                    # –ë–µ—Å–ø–ª–∞—Ç–Ω–∞—è, —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
    "deepseek/deepseek-r1": OpenAIProvider,                      # –ë–µ—Å–ø–ª–∞—Ç–Ω–∞—è, —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
    "google/gemini-flash-1.5": OpenAIProvider,                  # –ë–µ—Å–ø–ª–∞—Ç–Ω–∞—è, —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
    "qwen/qwen-2.5-coder-32b-instruct": OpenAIProvider,         # –ë–µ—Å–ø–ª–∞—Ç–Ω–∞—è, –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ, —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
    
    # === CLAUDE –ú–û–î–ï–õ–ò (—Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏) ===
    "anthropic/claude-3.5-sonnet": OpenAIProvider,              # –¢–æ–ø –º–æ–¥–µ–ª—å, —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
    "anthropic/claude-3.5-haiku": OpenAIProvider,               # –ë—ã—Å—Ç—Ä–∞—è, —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
    "anthropic/claude-3-opus": OpenAIProvider,                  # –ú–æ—â–Ω–∞—è, —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
    
    # === GOOGLE –ú–û–î–ï–õ–ò ===
    "google/gemini-pro-1.5": OpenAIProvider,                    # –° –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
    "google/gemini-pro": OpenAIProvider,                        # –° –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
    "google/gemma-2-27b-it": OpenAIProvider,                    # –ë–µ–∑ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
    
    # === DEEPSEEK –ú–û–î–ï–õ–ò ===
    "deepseek/deepseek-coder": OpenAIProvider,                  # –ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ, —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
    "deepseek/deepseek-reasoner": OpenAIProvider,               # –†–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
    
    # === QWEN –ú–û–î–ï–õ–ò ===
    "qwen/qwen-2.5-72b-instruct": OpenAIProvider,               # –° –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
    "qwen/qwen-2-vl-72b-instruct": OpenAIProvider,              # –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è, —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
    
    # === –õ–û–ö–ê–õ–¨–ù–´–ï –ú–û–î–ï–õ–ò (–±–µ–∑ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤) ===
    "llama3": LocalLLMProvider,
    "mistral": LocalLLMProvider,
    "codellama": LocalLLMProvider,
}

# –ú–æ–¥–µ–ª–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ (function calling)
MODELS_WITH_TOOLS = {
    # –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–µ —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
    "deepseek/deepseek-chat",
    "deepseek/deepseek-r1", 
    "google/gemini-flash-1.5",
    "qwen/qwen-2.5-coder-32b-instruct",
    
    # Claude —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
    "anthropic/claude-3.5-sonnet",
    "anthropic/claude-3.5-haiku", 
    "anthropic/claude-3-opus",
    
    # Google —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
    "google/gemini-pro-1.5",
    "google/gemini-pro",
    
    # DeepSeek —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
    "deepseek/deepseek-coder",
    "deepseek/deepseek-reasoner",
    
    # Qwen —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
    "qwen/qwen-2.5-72b-instruct",
    "qwen/qwen-2-vl-72b-instruct",
}

# –¢–æ–ª—å–∫–æ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ—Å—Ç–æ–≤
FREE_MODELS = {
    "deepseek/deepseek-chat",
    "deepseek/deepseek-r1",
    "google/gemini-flash-1.5", 
    "qwen/qwen-2.5-coder-32b-instruct",
}


def get_llm_provider(model: str = "auto", config: Optional[Config] = None) -> LLMProvider:
    """
    –ü–æ–ª—É—á–∏—Ç—å LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è –º–æ–¥–µ–ª–∏ (—Ç–æ–ª—å–∫–æ OpenRouter)
    
    Args:
        model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏–ª–∏ "auto" –¥–ª—è –∞–≤—Ç–æ–≤—ã–±–æ—Ä–∞
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        
    Returns:
        –ù–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π OpenRouter –ø—Ä–æ–≤–∞–π–¥–µ—Ä
    """
    config = config or get_config()
    
    if model == "auto":
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        model = config.default_model
    
    # –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º OpenAIProvider –¥–ª—è OpenRouter
    try:
        return OpenAIProvider(model, config)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è OpenRouter –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ –¥–ª—è {model}: {e}")
        # Fallback —Å mock –æ—Ç–≤–µ—Ç–∞–º–∏
        return MockLLMProvider()


def list_available_models() -> Dict[str, List[str]]:
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º"""
    return {
        "free": list(FREE_MODELS),
        "with_tools": list(MODELS_WITH_TOOLS),
        "all": list(MODEL_PROVIDERS.keys())
    }


def model_supports_tools(model: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ª–∏ –º–æ–¥–µ–ª—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã"""
    return model in MODELS_WITH_TOOLS


def get_free_models() -> List[str]:
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    return list(FREE_MODELS)


def get_best_free_model_with_tools() -> str:
    """–ü–æ–ª—É—á–∏—Ç—å –ª—É—á—à—É—é –±–µ—Å–ø–ª–∞—Ç–Ω—É—é –º–æ–¥–µ–ª—å —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤"""
    # DeepSeek Chat - –Ω–∞–∏–±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–∞—è –±–µ—Å–ø–ª–∞—Ç–Ω–∞—è –º–æ–¥–µ–ª—å —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
    return "deepseek/deepseek-chat"


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫–µ—à –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
_provider_cache: Dict[str, LLMProvider] = {}


def get_cached_provider(model: str, config: Optional[Config] = None) -> LLMProvider:
    """–ü–æ–ª—É—á–∏—Ç—å –ø—Ä–æ–≤–∞–π–¥–µ—Ä —Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    cache_key = f"{model}_{id(config) if config else 'default'}"
    
    if cache_key not in _provider_cache:
        _provider_cache[cache_key] = get_llm_provider(model, config)
    
    return _provider_cache[cache_key]


def clear_provider_cache():
    """–û—á–∏—Å—Ç–∏—Ç—å –∫–µ—à –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
    global _provider_cache
    _provider_cache.clear()


class MockLLMProvider(LLMProvider):
    """Mock –ø—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –±–µ–∑ API –∫–ª—é—á–µ–π"""
    
    def __init__(self, model: str = "mock"):
        # –°–æ–∑–¥–∞–µ–º —Ñ–∏–∫—Ç–∏–≤–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        from .config import Config
        mock_config = Config()
        super().__init__(model, mock_config)
    
    def complete(self, prompt: str, **kwargs) -> str:
        """Mock –æ—Ç–≤–µ—Ç"""
        return "Hello from KittyCore! (mock response)"
    
    def chat(self, messages: List[LLMMessage], **kwargs) -> LLMResponse:
        """Mock —á–∞—Ç"""
        return LLMResponse(
            content="Hello from KittyCore! (mock chat)",
            model=self.model,
            usage={"tokens": 10},
            metadata={"provider": "mock"}
        )
    
    def stream(self, prompt: str, **kwargs) -> Iterator[str]:
        """Mock —Å—Ç—Ä–∏–º–∏–Ω–≥"""
        words = ["Hello", "from", "KittyCore!", "(mock", "stream)"]
        for word in words:
            yield word + " " 