"""
ImageGenerationTool –¥–ª—è KittyCore 3.0
–ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ç–æ–ø–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ Replicate API 2025
"""

import asyncio
import base64
import json
import os
import tempfile
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass, asdict
from pathlib import Path
import httpx
import time

from .base_tool import BaseTool


@dataclass
class ImageGenerationRequest:
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    prompt: str
    model: Optional[str] = "flux-schnell"  # –î–µ—Ñ–æ–ª—Ç - —Å–∞–º–∞—è –¥–µ—à—ë–≤–∞—è –º–æ–¥–µ–ª—å
    width: Optional[int] = 1024
    height: Optional[int] = 1024
    num_images: Optional[int] = 1
    seed: Optional[int] = None
    style: Optional[str] = None
    negative_prompt: Optional[str] = None
    quality: Optional[str] = "standard"  # standard, high, ultra
    speed: Optional[str] = "balanced"    # fast, balanced, quality
    output_format: Optional[str] = "png"


@dataclass
class ImageGenerationResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    success: bool
    images: List[str]  # URLs –∏–ª–∏ base64
    model_used: str
    cost_estimate: float
    generation_time: float
    metadata: Dict[str, Any]
    error: Optional[str] = None


@dataclass
class ModelInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
    name: str
    replicate_id: str
    cost_per_image: float
    max_resolution: int
    supports_batch: bool
    speed_rating: int  # 1-5, –≥–¥–µ 5 - —Å–∞–º–∞—è –±—ã—Å—Ç—Ä–∞—è
    quality_rating: int  # 1-5, –≥–¥–µ 5 - –ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ
    description: str
    features: List[str]


class ImageGenerationTool(BaseTool):
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ Replicate API
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ç–æ–ø–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ 2025 –≥–æ–¥–∞:
    - FLUX —Å–µ–º–µ–π—Å—Ç–≤–æ (Schnell, Dev, Pro)
    - Google Imagen 4
    - Recraft V3
    - Ideogram V3
    - –ò –º–Ω–æ–≥–∏–µ –¥—Ä—É–≥–∏–µ
    """
    
    name = "image_generation"
    description = "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ AI –º–æ–¥–µ–ª–∏ (FLUX, Imagen, Ideogram, etc.)"
    
    # –¢–æ–ø–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ 2025 –≥–æ–¥–∞
    MODELS = {
        # FLUX —Å–µ–º–µ–π—Å—Ç–≤–æ - –ª—É—á—à–∏–π –±–∞–ª–∞–Ω—Å —Ü–µ–Ω—ã/–∫–∞—á–µ—Å—Ç–≤–∞
        "flux-schnell": ModelInfo(
            name="FLUX Schnell",
            replicate_id="black-forest-labs/flux-schnell",
            cost_per_image=0.003,  # $3/1000 images
            max_resolution=2048,
            supports_batch=True,
            speed_rating=5,
            quality_rating=4,
            description="–°–∞–º–∞—è –±—ã—Å—Ç—Ä–∞—è –∏ –¥–µ—à—ë–≤–∞—è –º–æ–¥–µ–ª—å FLUX",
            features=["ultra_fast", "batch_generation", "cost_effective"]
        ),
        
        "flux-dev": ModelInfo(
            name="FLUX Dev",
            replicate_id="black-forest-labs/flux-dev",
            cost_per_image=0.025,
            max_resolution=2048,
            supports_batch=True,
            speed_rating=4,
            quality_rating=4,
            description="–ë–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞ FLUX",
            features=["high_quality", "prompt_adherence", "creative"]
        ),
        
        "flux-pro": ModelInfo(
            name="FLUX 1.1 Pro",
            replicate_id="black-forest-labs/flux-1.1-pro",
            cost_per_image=0.04,
            max_resolution=4096,
            supports_batch=True,
            speed_rating=3,
            quality_rating=5,
            description="–¢–æ–ø–æ–≤–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ FLUX —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ–º–ø—Ç–∞–º",
            features=["sota_quality", "excellent_prompts", "high_res", "output_diversity"]
        ),
        
        # Google Imagen 4 - –Ω–æ–≤–∏–Ω–∫–∞ 2025
        "imagen-4": ModelInfo(
            name="Google Imagen 4",
            replicate_id="google/imagen-4",
            cost_per_image=0.08,  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è —Ü–µ–Ω–∞
            max_resolution=2048,
            supports_batch=False,
            speed_rating=3,
            quality_rating=5,
            description="–§–ª–∞–≥–º–∞–Ω—Å–∫–∞—è –º–æ–¥–µ–ª—å Google 2025",
            features=["google_quality", "photorealism", "fine_details"]
        ),
        
        "imagen-4-fast": ModelInfo(
            name="Google Imagen 4 Fast",
            replicate_id="google/imagen-4-fast",
            cost_per_image=0.04,
            max_resolution=2048,
            supports_batch=False,
            speed_rating=4,
            quality_rating=4,
            description="–ë—ã—Å—Ç—Ä–∞—è –≤–µ—Ä—Å–∏—è Imagen 4",
            features=["google_fast", "good_quality", "speed_optimized"]
        ),
        
        # Ideogram V3 - –ª—É—á—à–∏–π –¥–ª—è —Ç–µ–∫—Å—Ç–∞ –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö
        "ideogram-v3": ModelInfo(
            name="Ideogram V3 Quality",
            replicate_id="ideogram-ai/ideogram-v3-quality",
            cost_per_image=0.09,
            max_resolution=2048,
            supports_batch=False,
            speed_rating=3,
            quality_rating=5,
            description="–õ—É—á—à–∏–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö",
            features=["text_rendering", "creative_designs", "consistent_styles", "realism"]
        ),
        
        "ideogram-v3-turbo": ModelInfo(
            name="Ideogram V3 Turbo",
            replicate_id="ideogram-ai/ideogram-v3-turbo",
            cost_per_image=0.04,
            max_resolution=2048,
            supports_batch=False,
            speed_rating=5,
            quality_rating=4,
            description="–ë—ã—Å—Ç—Ä–∞—è –≤–µ—Ä—Å–∏—è Ideogram V3",
            features=["text_rendering", "fast_generation", "cost_effective"]
        ),
        
        # Recraft V3 - SOTA –∫–∞—á–µ—Å—Ç–≤–æ
        "recraft-v3": ModelInfo(
            name="Recraft V3",
            replicate_id="recraft-ai/recraft-v3",
            cost_per_image=0.04,
            max_resolution=2048,
            supports_batch=False,
            speed_rating=3,
            quality_rating=5,
            description="SOTA –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ Artificial Analysis benchmark",
            features=["sota_benchmark", "long_texts", "wide_styles", "design_focused"]
        ),
        
        # Stable Diffusion XL - –∫–ª–∞—Å—Å–∏–∫–∞
        "sdxl": ModelInfo(
            name="Stable Diffusion XL",
            replicate_id="stability-ai/sdxl",
            cost_per_image=0.02,
            max_resolution=1024,
            supports_batch=True,
            speed_rating=4,
            quality_rating=3,
            description="–ü—Ä–æ–≤–µ—Ä–µ–Ω–Ω–∞—è –≤—Ä–µ–º–µ–Ω–µ–º –º–æ–¥–µ–ª—å",
            features=["open_source", "customizable", "community_support"]
        )
    }
    
    def __init__(self, replicate_api_token: Optional[str] = None):
        super().__init__()
        self.api_token = replicate_api_token or os.getenv("REPLICATE_API_TOKEN")
        self.base_url = "https://api.replicate.com/v1"
        self.client = httpx.AsyncClient()
        
        if not self.api_token:
            self.logger.warning("‚ö†Ô∏è REPLICATE_API_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω - –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –¥–µ–º–æ —Ä–µ–∂–∏–º")
    
    async def execute(self, action: str, **kwargs) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        try:
            if action == "generate_image":
                return await self._generate_image(**kwargs)
            elif action == "list_models":
                return await self._list_models(**kwargs)
            elif action == "get_model_info":
                return await self._get_model_info(**kwargs)
            elif action == "estimate_cost":
                return await self._estimate_cost(**kwargs)
            elif action == "batch_generate":
                return await self._batch_generate(**kwargs)
            elif action == "auto_select_model":
                return await self._auto_select_model(**kwargs)
            else:
                return {
                    "success": False,
                    "error": f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ: {action}",
                    "available_actions": [
                        "generate_image", "list_models", "get_model_info", 
                        "estimate_cost", "batch_generate", "auto_select_model"
                    ]
                }
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ ImageGenerationTool.{action}: {e}")
            return {"success": False, "error": str(e)}
    
    async def _generate_image(
        self, 
        prompt: str,
        model: Optional[str] = None,
        width: int = 1024,
        height: int = 1024,
        num_images: int = 1,
        seed: Optional[int] = None,
        style: Optional[str] = None,
        negative_prompt: Optional[str] = None,
        quality: str = "standard",
        speed: str = "balanced",
        output_format: str = "png",
        save_path: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ –≤—ã–±—Ä–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å"""
        
        start_time = time.time()
        
        # –ê–≤—Ç–æ–≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–∞
        if not model:
            model = await self._smart_model_selection(prompt, quality, speed)
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        if model not in self.MODELS:
            return {
                "success": False,
                "error": f"–ú–æ–¥–µ–ª—å '{model}' –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è",
                "available_models": list(self.MODELS.keys())
            }
        
        model_info = self.MODELS[model]
        
        # –î–µ–º–æ —Ä–µ–∂–∏–º –±–µ–∑ API —Ç–æ–∫–µ–Ω–∞
        if not self.api_token:
            return await self._demo_mode_response(prompt, model, model_info)
        
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞
            request_data = await self._prepare_request(
                model_info, prompt, width, height, num_images, 
                seed, style, negative_prompt, output_format, **kwargs
            )
            
            # –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ Replicate
            result = await self._call_replicate_api(request_data)
            
            generation_time = time.time() - start_time
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            if result["success"]:
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω –ø—É—Ç—å
                if save_path:
                    saved_paths = await self._save_images(result["images"], save_path, output_format)
                    result["saved_paths"] = saved_paths
                
                result.update({
                    "model_used": model,
                    "model_info": asdict(model_info),
                    "generation_time": generation_time,
                    "cost_estimate": model_info.cost_per_image * num_images,
                    "prompt_used": prompt
                })
            
            return result
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            return {
                "success": False,
                "error": str(e),
                "model": model,
                "generation_time": time.time() - start_time
            }
    
    async def _smart_model_selection(
        self, 
        prompt: str, 
        quality: str, 
        speed: str
    ) -> str:
        """–£–º–Ω—ã–π –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π"""
        
        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–º–ø—Ç–∞ –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        prompt_lower = prompt.lower()
        
        # –ï—Å–ª–∏ –Ω—É–∂–µ–Ω —Ç–µ–∫—Å—Ç –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ - Ideogram
        if any(word in prompt_lower for word in ["text", "sign", "logo", "writing", "letter", "word"]):
            return "ideogram-v3-turbo" if speed == "fast" else "ideogram-v3"
        
        # –ï—Å–ª–∏ –Ω—É–∂–Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å
        if speed == "fast":
            return "flux-schnell"
        
        # –ï—Å–ª–∏ –Ω—É–∂–Ω–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
        if quality == "ultra":
            return "flux-pro"
        
        # –ï—Å–ª–∏ –Ω—É–∂–µ–Ω —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏–∑–º
        if any(word in prompt_lower for word in ["photo", "realistic", "portrait", "person", "face"]):
            return "imagen-4-fast" if speed == "fast" else "imagen-4"
        
        # –î–µ—Ñ–æ–ª—Ç - –ª—É—á—à–∏–π –±–∞–ª–∞–Ω—Å
        return "flux-dev"
    
    async def _prepare_request(
        self, 
        model_info: ModelInfo, 
        prompt: str, 
        width: int, 
        height: int, 
        num_images: int,
        seed: Optional[int],
        style: Optional[str],
        negative_prompt: Optional[str],
        output_format: str,
        **kwargs
    ) -> Dict[str, Any]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        
        # –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        inputs = {
            "prompt": prompt,
            "width": min(width, model_info.max_resolution),
            "height": min(height, model_info.max_resolution),
            "output_format": output_format.lower()
        }
        
        # Seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        if seed is not None:
            inputs["seed"] = seed
        
        # Negative prompt –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è
        if negative_prompt:
            inputs["negative_prompt"] = negative_prompt
        
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–µ—Å–ª–∏ –º–æ–¥–µ–ª—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç)
        if model_info.supports_batch and num_images > 1:
            inputs["num_outputs"] = min(num_images, 4)  # –ú–∞–∫—Å–∏–º—É–º 4 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        
        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è FLUX –º–æ–¥–µ–ª–µ–π
        if "flux" in model_info.replicate_id:
            inputs.update({
                "guidance_scale": kwargs.get("guidance_scale", 3.5),
                "num_inference_steps": kwargs.get("steps", 28),
                "max_sequence_length": kwargs.get("max_sequence_length", 512)
            })
        
        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è Ideogram
        elif "ideogram" in model_info.replicate_id:
            inputs.update({
                "style_type": style or "AUTO",
                "magic_prompt_option": kwargs.get("magic_prompt", "AUTO")
            })
        
        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è Imagen
        elif "imagen" in model_info.replicate_id:
            inputs.update({
                "aspect_ratio": f"{width}:{height}",
                "safety_tolerance": kwargs.get("safety_tolerance", 2)
            })
        
        return {
            "version": model_info.replicate_id,
            "input": inputs
        }
    
    async def _call_replicate_api(self, request_data: Dict[str, Any]) -> Dict[str, Any]:
        """–í—ã–∑–æ–≤ Replicate API"""
        
        headers = {
            "Authorization": f"Token {self.api_token}",
            "Content-Type": "application/json"
        }
        
        # –°–æ–∑–¥–∞–Ω–∏–µ prediction
        response = await self.client.post(
            f"{self.base_url}/predictions",
            headers=headers,
            json=request_data,
            timeout=30.0
        )
        
        if response.status_code != 201:
            return {
                "success": False,
                "error": f"API error: {response.status_code} - {response.text}"
            }
        
        prediction = response.json()
        prediction_id = prediction["id"]
        
        # –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        max_wait = 300  # 5 –º–∏–Ω—É—Ç –º–∞–∫—Å–∏–º—É–º
        wait_time = 0
        
        while wait_time < max_wait:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞
            status_response = await self.client.get(
                f"{self.base_url}/predictions/{prediction_id}",
                headers=headers,
                timeout=10.0
            )
            
            if status_response.status_code != 200:
                return {
                    "success": False,
                    "error": f"Status check error: {status_response.status_code}"
                }
            
            status_data = status_response.json()
            status = status_data["status"]
            
            if status == "succeeded":
                return {
                    "success": True,
                    "images": status_data["output"] if isinstance(status_data["output"], list) else [status_data["output"]],
                    "metadata": {
                        "prediction_id": prediction_id,
                        "processing_time": status_data.get("metrics", {}).get("predict_time"),
                        "model_version": status_data.get("version")
                    }
                }
            
            elif status == "failed":
                return {
                    "success": False,
                    "error": f"Generation failed: {status_data.get('error', 'Unknown error')}"
                }
            
            # –û–∂–∏–¥–∞–Ω–∏–µ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π
            await asyncio.sleep(2)
            wait_time += 2
        
        return {
            "success": False,
            "error": "Generation timeout - –ø—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è"
        }
    
    async def _demo_mode_response(
        self, 
        prompt: str, 
        model: str, 
        model_info: ModelInfo
    ) -> Dict[str, Any]:
        """–î–µ–º–æ —Ä–µ–∂–∏–º –±–µ–∑ —Ä–µ–∞–ª—å–Ω–æ–≥–æ API"""
        
        # –ò–º–∏—Ç–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        await asyncio.sleep(2)
        
        return {
            "success": True,
            "images": [
                "https://picsum.photos/1024/1024?random=1",  # Placeholder –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mNkYPhfDwAChwGA60e6kgAAAABJRU5ErkJggg=="
            ],
            "model_used": model,
            "model_info": asdict(model_info),
            "generation_time": 2.0,
            "cost_estimate": model_info.cost_per_image,
            "prompt_used": prompt,
            "demo_mode": True,
            "message": "üé≠ –î–ï–ú–û –†–ï–ñ–ò–ú: –†–µ–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç—Ä–µ–±—É–µ—Ç REPLICATE_API_TOKEN",
            "metadata": {
                "demo": True,
                "placeholder_used": True
            }
        }
    
    async def _save_images(
        self, 
        image_urls: List[str], 
        save_path: str, 
        output_format: str
    ) -> List[str]:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –¥–∏—Å–∫"""
        
        saved_paths = []
        save_dir = Path(save_path)
        save_dir.mkdir(parents=True, exist_ok=True)
        
        for i, url in enumerate(image_urls):
            try:
                # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
                timestamp = int(time.time())
                filename = f"generated_image_{timestamp}_{i}.{output_format}"
                file_path = save_dir / filename
                
                # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                if url.startswith("data:"):
                    # Base64 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                    header, data = url.split(",", 1)
                    image_data = base64.b64decode(data)
                    file_path.write_bytes(image_data)
                else:
                    # URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                    response = await self.client.get(url, timeout=30.0)
                    if response.status_code == 200:
                        file_path.write_bytes(response.content)
                    else:
                        continue
                
                saved_paths.append(str(file_path))
                self.logger.info(f"üíæ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {file_path}")
                
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {i}: {e}")
        
        return saved_paths
    
    async def _list_models(self, category: Optional[str] = None) -> Dict[str, Any]:
        """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        
        models = {}
        
        for model_id, model_info in self.MODELS.items():
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            if category:
                if category.lower() == "fast" and model_info.speed_rating < 4:
                    continue
                elif category.lower() == "quality" and model_info.quality_rating < 4:
                    continue
                elif category.lower() == "cheap" and model_info.cost_per_image > 0.03:
                    continue
            
            models[model_id] = {
                "name": model_info.name,
                "description": model_info.description,
                "cost_per_image": model_info.cost_per_image,
                "speed_rating": model_info.speed_rating,
                "quality_rating": model_info.quality_rating,
                "max_resolution": model_info.max_resolution,
                "supports_batch": model_info.supports_batch,
                "features": model_info.features
            }
        
        return {
            "success": True,
            "models": models,
            "total_models": len(models),
            "categories": ["fast", "quality", "cheap"],
            "recommendations": {
                "cheapest": "flux-schnell",
                "fastest": "flux-schnell",
                "best_quality": "flux-pro",
                "best_text": "ideogram-v3",
                "balanced": "flux-dev"
            }
        }
    
    async def _get_model_info(self, model: str) -> Dict[str, Any]:
        """–ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏"""
        
        if model not in self.MODELS:
            return {
                "success": False,
                "error": f"–ú–æ–¥–µ–ª—å '{model}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞",
                "available_models": list(self.MODELS.keys())
            }
        
        model_info = self.MODELS[model]
        
        return {
            "success": True,
            "model": model,
            "info": asdict(model_info),
            "pricing": {
                "cost_per_image": model_info.cost_per_image,
                "cost_for_10": model_info.cost_per_image * 10,
                "cost_for_100": model_info.cost_per_image * 100,
                "monthly_estimate_100_images": model_info.cost_per_image * 100
            },
            "capabilities": {
                "max_batch_size": 4 if model_info.supports_batch else 1,
                "estimated_time_per_image": 30 / model_info.speed_rating,  # —Å–µ–∫—É–Ω–¥
                "recommended_for": model_info.features
            }
        }
    
    async def _estimate_cost(
        self, 
        model: str, 
        num_images: int = 1, 
        monthly_usage: Optional[int] = None
    ) -> Dict[str, Any]:
        """–û—Ü–µ–Ω–∫–∞ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
        
        if model not in self.MODELS:
            return {
                "success": False,
                "error": f"–ú–æ–¥–µ–ª—å '{model}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
            }
        
        model_info = self.MODELS[model]
        
        single_cost = model_info.cost_per_image
        batch_cost = single_cost * num_images
        
        estimates = {
            "single_image": single_cost,
            "current_batch": batch_cost,
            "daily_10_images": single_cost * 10,
            "weekly_50_images": single_cost * 50,
            "monthly_200_images": single_cost * 200
        }
        
        if monthly_usage:
            estimates["monthly_custom"] = single_cost * monthly_usage
        
        return {
            "success": True,
            "model": model,
            "cost_per_image": single_cost,
            "estimates": estimates,
            "currency": "USD",
            "comparison": {
                "vs_midjourney": f"{(batch_cost / 0.05):.1f}x cheaper" if batch_cost < 0.05 else f"{(0.05 / batch_cost):.1f}x more expensive",
                "vs_dalle3": f"{(batch_cost / 0.08):.1f}x cheaper" if batch_cost < 0.08 else f"{(0.08 / batch_cost):.1f}x more expensive"
            }
        }
    
    async def _batch_generate(
        self, 
        prompts: List[str],
        model: Optional[str] = None,
        **common_params
    ) -> Dict[str, Any]:
        """–ë–∞—Ç—á–µ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        
        if not prompts:
            return {
                "success": False,
                "error": "–°–ø–∏—Å–æ–∫ –ø—Ä–æ–º–ø—Ç–æ–≤ –ø—É—Å—Ç"
            }
        
        if len(prompts) > 10:
            return {
                "success": False,
                "error": "–ú–∞–∫—Å–∏–º—É–º 10 –ø—Ä–æ–º–ø—Ç–æ–≤ –≤ –æ–¥–Ω–æ–º –±–∞—Ç—á–µ"
            }
        
        results = []
        total_cost = 0
        total_time = 0
        
        for i, prompt in enumerate(prompts):
            self.logger.info(f"üé® –ì–µ–Ω–µ—Ä–∞—Ü–∏—è {i+1}/{len(prompts)}: {prompt[:50]}...")
            
            result = await self._generate_image(
                prompt=prompt,
                model=model,
                **common_params
            )
            
            results.append(result)
            
            if result["success"]:
                total_cost += result.get("cost_estimate", 0)
                total_time += result.get("generation_time", 0)
        
        successful = [r for r in results if r["success"]]
        failed = [r for r in results if not r["success"]]
        
        return {
            "success": len(successful) > 0,
            "results": results,
            "summary": {
                "total_prompts": len(prompts),
                "successful": len(successful),
                "failed": len(failed),
                "total_cost": total_cost,
                "total_time": total_time,
                "average_time_per_image": total_time / max(len(successful), 1)
            },
            "all_images": [img for result in successful for img in result.get("images", [])],
            "errors": [r["error"] for r in failed]
        }
    
    async def _auto_select_model(
        self, 
        prompt: str,
        budget: Optional[float] = None,
        priority: str = "balanced"  # "speed", "quality", "cost", "balanced"
    ) -> Dict[str, Any]:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        
        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–º–ø—Ç–∞
        prompt_analysis = await self._analyze_prompt(prompt)
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π –ø–æ –±—é–¥–∂–µ—Ç—É
        available_models = self.MODELS
        if budget:
            available_models = {
                k: v for k, v in self.MODELS.items() 
                if v.cost_per_image <= budget
            }
        
        if not available_models:
            return {
                "success": False,
                "error": f"–ù–µ—Ç –º–æ–¥–µ–ª–µ–π –≤ –±—é–¥–∂–µ—Ç–µ ${budget}",
                "cheapest_model": min(self.MODELS.values(), key=lambda x: x.cost_per_image).name
            }
        
        # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        if priority == "speed":
            best_model = max(available_models.items(), key=lambda x: x[1].speed_rating)
        elif priority == "quality":
            best_model = max(available_models.items(), key=lambda x: x[1].quality_rating)
        elif priority == "cost":
            best_model = min(available_models.items(), key=lambda x: x[1].cost_per_image)
        else:  # balanced
            best_model = max(
                available_models.items(), 
                key=lambda x: (x[1].speed_rating + x[1].quality_rating) / x[1].cost_per_image
            )
        
        model_id, model_info = best_model
        
        return {
            "success": True,
            "recommended_model": model_id,
            "model_info": asdict(model_info),
            "reasoning": {
                "priority": priority,
                "budget_constraint": budget,
                "prompt_analysis": prompt_analysis,
                "why_selected": self._explain_selection(model_info, priority, prompt_analysis)
            },
            "alternatives": [
                {"model": k, "score": v.speed_rating + v.quality_rating, "cost": v.cost_per_image}
                for k, v in list(available_models.items())[:3]
            ]
        }
    
    async def _analyze_prompt(self, prompt: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –≤—ã–±–æ—Ä–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        
        prompt_lower = prompt.lower()
        
        analysis = {
            "needs_text_rendering": any(word in prompt_lower for word in ["text", "sign", "logo", "writing", "letter"]),
            "is_photorealistic": any(word in prompt_lower for word in ["photo", "realistic", "portrait", "person"]),
            "is_artistic": any(word in prompt_lower for word in ["art", "painting", "drawing", "illustration"]),
            "is_technical": any(word in prompt_lower for word in ["diagram", "schematic", "technical", "blueprint"]),
            "complexity": "high" if len(prompt.split()) > 20 else "medium" if len(prompt.split()) > 10 else "simple"
        }
        
        return analysis
    
    def _explain_selection(
        self, 
        model_info: ModelInfo, 
        priority: str, 
        analysis: Dict[str, Any]
    ) -> str:
        """–û–±—ä—è—Å–Ω–µ–Ω–∏–µ –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏"""
        
        reasons = []
        
        if analysis["needs_text_rendering"] and "text_rendering" in model_info.features:
            reasons.append("–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞")
        
        if analysis["is_photorealistic"] and "photorealism" in model_info.features:
            reasons.append("–æ—Ç–ª–∏—á–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏–∑–º–∞")
        
        if priority == "speed" and model_info.speed_rating >= 4:
            reasons.append("–≤—ã—Å–æ–∫–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")
        
        if priority == "quality" and model_info.quality_rating >= 4:
            reasons.append("–ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ")
        
        if priority == "cost" and model_info.cost_per_image <= 0.03:
            reasons.append("–æ—á–µ–Ω—å –Ω–∏–∑–∫–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å")
        
        if not reasons:
            reasons.append("–ª—É—á—à–∏–π –±–∞–ª–∞–Ω—Å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫")
        
        return ", ".join(reasons)
    
    async def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        if hasattr(self, 'client'):
            await self.client.aclose()
    
    def get_json_schema(self) -> Dict[str, Any]:
        """JSON Schema –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        return {
            "type": "object",
            "properties": {
                "action": {
                    "type": "string",
                    "enum": [
                        "generate_image", "list_models", "get_model_info", 
                        "estimate_cost", "batch_generate", "auto_select_model"
                    ],
                    "description": "–î–µ–π—Å—Ç–≤–∏–µ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"
                },
                "prompt": {
                    "type": "string",
                    "description": "–û–ø–∏—Å–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏",
                    "minLength": 1,
                    "maxLength": 1000
                },
                "model": {
                    "type": "string",
                    "enum": list(MODELS.keys()),
                    "description": "–ú–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"
                },
                "width": {
                    "type": "integer",
                    "minimum": 256,
                    "maximum": 4096,
                    "default": 1024
                },
                "height": {
                    "type": "integer", 
                    "minimum": 256,
                    "maximum": 4096,
                    "default": 1024
                },
                "num_images": {
                    "type": "integer",
                    "minimum": 1,
                    "maximum": 4,
                    "default": 1
                },
                "quality": {
                    "type": "string",
                    "enum": ["standard", "high", "ultra"],
                    "default": "standard"
                },
                "speed": {
                    "type": "string",
                    "enum": ["fast", "balanced", "quality"],
                    "default": "balanced"
                }
            },
            "required": ["action"],
            "additionalProperties": True
        }


# –≠–∫—Å–ø–æ—Ä—Ç –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –∞–≥–µ–Ω—Ç–∞—Ö
__all__ = ["ImageGenerationTool", "ImageGenerationRequest", "ImageGenerationResult", "ModelInfo"]