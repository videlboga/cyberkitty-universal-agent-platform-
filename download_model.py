#!/usr/bin/env python3
"""
üì• –ü–†–Ø–ú–ê–Ø –ó–ê–ì–†–£–ó–ö–ê EMBEDDING –ú–û–î–ï–õ–ò
"""

import time
from huggingface_hub import hf_hub_download
import os

def download_model():
    """–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞–ø—Ä—è–º—É—é"""
    print("üöÄ –ó–ê–ì–†–£–ñ–ê–ï–ú –ú–û–î–ï–õ–¨ –ù–ê–ü–†–Ø–ú–£–Æ...")
    
    model_name = "sentence-transformers/paraphrase-MiniLM-L3-v2"
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–∏
    files_to_download = [
        "config.json",
        "pytorch_model.bin",
        "tokenizer.json", 
        "tokenizer_config.json",
        "vocab.txt",
        "modules.json",
        "sentence_bert_config.json"
    ]
    
    start_time = time.time()
    
    for filename in files_to_download:
        try:
            print(f"üì¶ –ó–∞–≥—Ä—É–∂–∞–µ–º {filename}...")
            
            file_path = hf_hub_download(
                repo_id=model_name,
                filename=filename,
                cache_dir="./hf_cache"
            )
            
            file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB
            print(f"‚úÖ {filename} –∑–∞–≥—Ä—É–∂–µ–Ω ({file_size:.1f} MB)")
            
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {filename}: {e}")
    
    total_time = time.time() - start_time
    print(f"\n‚è±Ô∏è –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {total_time:.2f}—Å")
    
    # –¢–µ–ø–µ—Ä—å —Ç–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
    test_model()

def test_model():
    """–¢–µ—Å—Ç–∏—Ä—É–µ–º –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å"""
    print("\nüß™ –¢–ï–°–¢–ò–†–£–ï–ú –ó–ê–ì–†–£–ñ–ï–ù–ù–£–Æ –ú–û–î–ï–õ–¨...")
    
    try:
        # –£–∫–∞–∑—ã–≤–∞–µ–º –ø—É—Ç—å –∫ –∫–µ—à—É
        os.environ['HF_HOME'] = './hf_cache'
        
        from sentence_transformers import SentenceTransformer
        
        start = time.time()
        model = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L3-v2')
        end = time.time()
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {end-start:.2f}—Å")
        print(f"üìä –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {model.get_sentence_embedding_dimension()}")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        texts = ["–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ", "–≤–µ–± —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞", "–±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"]
        embeddings = model.encode(texts)
        
        print(f"üß† –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ–∑–¥–∞–Ω—ã: {embeddings.shape}")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –±–ª–∏–∑–æ—Å—Ç—å
        from sentence_transformers.util import cos_sim
        
        similarity = cos_sim(embeddings[0], embeddings[1])
        print(f"üîó –°—Ö–æ–¥—Å—Ç–≤–æ '–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ' vs '–≤–µ–± —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞': {similarity.item():.3f}")
        
        print("üéâ –ú–û–î–ï–õ–¨ –†–ê–ë–û–¢–ê–ï–¢ –ò–î–ï–ê–õ–¨–ù–û!")
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        return False

if __name__ == "__main__":
    download_model() 