"""
üìö A-MEM Document Ingestion - –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ A-MEM –¥–ª—è RAG —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞

–í–º–µ—Å—Ç–æ —Å–æ–∑–¥–∞–Ω–∏—è –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ VectorSearchTool, —Ä–∞—Å—à–∏—Ä—è–µ–º A-MEM 
–¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–Ω–µ—à–Ω–∏–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –∏ —Ñ–∞–π–ª–∞–º–∏.

–ü—Ä–∏–Ω—Ü–∏–ø: "–û–¥–∏–Ω –º–æ–∑–≥ –¥–ª—è –≤—Å–µ–≥–æ - –∞–≥–µ–Ω—Ç–Ω–∞—è –ø–∞–º—è—Ç—å + –¥–æ–∫—É–º–µ–Ω—Ç—ã" üß†üìö
"""

import asyncio
import logging
import os
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
from datetime import datetime
import hashlib

try:
    import PyPDF2
    import docx
    from bs4 import BeautifulSoup
    DOC_DEPS_AVAILABLE = True
except ImportError:
    DOC_DEPS_AVAILABLE = False

from .amem_integration import KittyCoreMemorySystem, get_enhanced_memory_system

logger = logging.getLogger(__name__)

class DocumentProcessor:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    
    @staticmethod
    def extract_text_from_pdf(file_path: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF"""
        if not DOC_DEPS_AVAILABLE:
            raise ImportError("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: pip install PyPDF2 python-docx beautifulsoup4")
        
        try:
            with open(file_path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                text = ""
                for page in reader.pages:
                    text += page.extract_text() + "\n"
                return text.strip()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF {file_path}: {e}")
            return ""
    
    @staticmethod
    def extract_text_from_docx(file_path: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ DOCX"""
        if not DOC_DEPS_AVAILABLE:
            raise ImportError("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: pip install python-docx")
        
        try:
            doc = docx.Document(file_path)
            text = ""
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
            return text.strip()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è DOCX {file_path}: {e}")
            return ""
    
    @staticmethod
    def extract_text_from_html(file_path: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ HTML"""
        if not DOC_DEPS_AVAILABLE:
            raise ImportError("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: pip install beautifulsoup4")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                soup = BeautifulSoup(file.read(), 'html.parser')
                # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
                for script in soup(["script", "style"]):
                    script.decompose()
                return soup.get_text().strip()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è HTML {file_path}: {e}")
            return ""
    
    @staticmethod
    def extract_text_from_txt(file_path: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ TXT"""
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                return file.read().strip()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è TXT {file_path}: {e}")
            return ""

class AMEMDocumentIngestion:
    """
    üß†üìö –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ A-MEM –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
    
    –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –≤–Ω–µ—à–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –∞–≥–µ–Ω—Ç–Ω—É—é –ø–∞–º—è—Ç—å –¥–ª—è RAG —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞.
    –ü—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π VectorSearchTool —á–µ—Ä–µ–∑ A-MEM –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏.
    """
    
    def __init__(self, vault_path: str = "obsidian_vault"):
        self.memory_system = get_enhanced_memory_system(vault_path)
        self.document_processor = DocumentProcessor()
        self.vault_path = Path(vault_path)
        
        # –ü–∞–ø–∫–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        self.documents_folder = self.vault_path / "knowledge"
        self.documents_folder.mkdir(exist_ok=True)
        
        logger.info(f"üìö A-MEM Document Ingestion –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {vault_path}")
    
    async def ingest_file(self, file_path: str, 
                         category: str = "document",
                         metadata: Dict[str, Any] = None) -> str:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ –≤ A-MEM –ø–∞–º—è—Ç—å
        
        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞  
            metadata: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        
        Returns:
            memory_id: ID —Å–æ–∑–¥–∞–Ω–Ω–æ–π –∑–∞–ø–∏—Å–∏ –≤ –ø–∞–º—è—Ç–∏
        """
        try:
            file_path = Path(file_path)
            
            if not file_path.exists():
                raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞
            text_content = await self._extract_text(file_path)
            
            if not text_content:
                raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ {file_path}")
            
            # –ì–æ—Ç–æ–≤–∏–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            file_metadata = {
                "source_file": str(file_path),
                "file_name": file_path.name,
                "file_size": file_path.stat().st_size,
                "file_extension": file_path.suffix,
                "file_hash": self._calculate_file_hash(file_path),
                "ingestion_date": datetime.now().isoformat(),
                "category": category,
                **(metadata or {})
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ A-MEM –∫–∞–∫ –ø–∞–º—è—Ç—å –∞–≥–µ–Ω—Ç–∞-–¥–æ–∫—É–º–µ–Ω—Ç–∞
            memory_id = await self.memory_system.agent_remember(
                agent_id=f"document_{file_path.stem}",
                memory=text_content,
                context=file_metadata
            )
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–ø–∏—é –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ knowledge base
            await self._save_to_knowledge_base(file_path, text_content, file_metadata)
            
            logger.info(f"üìö –î–æ–∫—É–º–µ–Ω—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –≤ A-MEM: {file_path.name} ‚Üí {memory_id}")
            return memory_id
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {file_path}: {e}")
            raise
    
    async def ingest_directory(self, 
                             directory_path: str,
                             file_patterns: List[str] = None,
                             recursive: bool = True) -> List[str]:
        """
        –ú–∞—Å—Å–æ–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –ø–∞–ø–∫–∏
        
        Args:
            directory_path: –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ
            file_patterns: –ü–∞—Ç—Ç–µ—Ä–Ω—ã —Ñ–∞–π–ª–æ–≤ (*.pdf, *.docx, etc.)
            recursive: –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫
        
        Returns:
            List[memory_id]: ID —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
        """
        try:
            directory_path = Path(directory_path)
            
            if not directory_path.exists():
                raise FileNotFoundError(f"–ü–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {directory_path}")
            
            # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            if file_patterns is None:
                file_patterns = ["*.pdf", "*.docx", "*.txt", "*.html", "*.md"]
            
            # –ü–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤
            found_files = []
            for pattern in file_patterns:
                if recursive:
                    found_files.extend(directory_path.rglob(pattern))
                else:
                    found_files.extend(directory_path.glob(pattern))
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤
            memory_ids = []
            for file_path in found_files:
                try:
                    memory_id = await self.ingest_file(
                        str(file_path),
                        category="batch_upload",
                        metadata={"batch_source": str(directory_path)}
                    )
                    memory_ids.append(memory_id)
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file_path}: {e}")
                    continue
            
            logger.info(f"üìö –ú–∞—Å—Å–æ–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(memory_ids)}/{len(found_files)} —Ñ–∞–π–ª–æ–≤")
            return memory_ids
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –º–∞—Å—Å–æ–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑ {directory_path}: {e}")
            raise
    
    async def search_documents(self, 
                             query: str, 
                             category: str = None,
                             limit: int = 5) -> List[Dict[str, Any]]:
        """
        –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º —á–µ—Ä–µ–∑ A-MEM
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            category: –§–∏–ª—å—Ç—Ä –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            limit: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
        Returns:
            List[Dict]: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        try:
            # –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ A-MEM –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω—É—é –ø–∞–º—è—Ç—å
            results = await self.memory_system.collective_search(
                query=query,
                team_id="documents"  # –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∫–∞–∫ –æ–¥–Ω–∞ –∫–æ–º–∞–Ω–¥–∞
            )
            
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞
            if category:
                filtered_results = []
                for result in results:
                    context = result.get('context', {})
                    if context.get('category') == category:
                        filtered_results.append(result)
                results = filtered_results
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            return results[:limit]
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            return []
    
    async def get_document_stats(self) -> Dict[str, Any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        try:
            # –ü–æ–∏—Å–∫ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            all_docs = await self.search_documents("", limit=1000)
            
            # –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            stats = {
                "total_documents": len(all_docs),
                "categories": {},
                "file_types": {},
                "total_size": 0,
                "latest_ingestion": None
            }
            
            for doc in all_docs:
                context = doc.get('context', {})
                
                # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏
                category = context.get('category', 'unknown')
                stats["categories"][category] = stats["categories"].get(category, 0) + 1
                
                # –¢–∏–ø—ã —Ñ–∞–π–ª–æ–≤
                file_ext = context.get('file_extension', 'unknown')
                stats["file_types"][file_ext] = stats["file_types"].get(file_ext, 0) + 1
                
                # –†–∞–∑–º–µ—Ä
                file_size = context.get('file_size', 0)
                stats["total_size"] += file_size
                
                # –ü–æ—Å–ª–µ–¥–Ω—è—è –∑–∞–≥—Ä—É–∑–∫–∞
                ingestion_date = context.get('ingestion_date')
                if ingestion_date:
                    if not stats["latest_ingestion"] or ingestion_date > stats["latest_ingestion"]:
                        stats["latest_ingestion"] = ingestion_date
            
            return stats
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            return {"error": str(e)}
    
    async def _extract_text(self, file_path: Path) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é"""
        extension = file_path.suffix.lower()
        
        if extension == '.pdf':
            return self.document_processor.extract_text_from_pdf(str(file_path))
        elif extension == '.docx':
            return self.document_processor.extract_text_from_docx(str(file_path))
        elif extension in ['.html', '.htm']:
            return self.document_processor.extract_text_from_html(str(file_path))
        elif extension in ['.txt', '.md']:
            return self.document_processor.extract_text_from_txt(str(file_path))
        else:
            # –ü—Ä–æ–±—É–µ–º –∫–∞–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª
            return self.document_processor.extract_text_from_txt(str(file_path))
    
    def _calculate_file_hash(self, file_path: Path) -> str:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ö–µ—à–∞ —Ñ–∞–π–ª–∞ –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    async def _save_to_knowledge_base(self, 
                                    file_path: Path, 
                                    text_content: str,
                                    metadata: Dict[str, Any]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ knowledge base vault"""
        try:
            # –°–æ–∑–¥–∞—ë–º markdown —Ñ–∞–π–ª –≤ knowledge –ø–∞–ø–∫–µ
            output_path = self.documents_folder / f"{file_path.stem}.md"
            
            markdown_content = f"""# {file_path.name}

## –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
- **–ò—Å—Ç–æ—á–Ω–∏–∫**: {metadata.get('source_file', 'unknown')}
- **–†–∞–∑–º–µ—Ä**: {metadata.get('file_size', 0)} –±–∞–π—Ç
- **–¢–∏–ø**: {metadata.get('file_extension', 'unknown')}
- **–ó–∞–≥—Ä—É–∂–µ–Ω–æ**: {metadata.get('ingestion_date', 'unknown')}
- **–•–µ—à**: {metadata.get('file_hash', 'unknown')}

## –°–æ–¥–µ—Ä–∂–∏–º–æ–µ

{text_content}

---
*–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ A-MEM Document Ingestion*
"""
            
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(markdown_content)
            
            logger.info(f"üíæ –î–æ–∫—É–º–µ–Ω—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ knowledge base: {output_path}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ knowledge base: {e}")


# === –§–ê–ë–†–ò–ß–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø ===

def get_amem_document_ingestion(vault_path: str = "obsidian_vault") -> AMEMDocumentIngestion:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ A-MEM"""
    return AMEMDocumentIngestion(vault_path)