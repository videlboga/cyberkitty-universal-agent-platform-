"""
ğŸŒ Web Common - ĞĞ±Ñ‰Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ²ĞµĞ±-Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² KittyCore 3.0

ĞĞ±Ñ‰Ğ¸Ğµ Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ñ‹, ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑƒÑ‚Ğ¸Ğ»Ğ¸Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ²ĞµĞ±-Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²
"""

import asyncio
import aiohttp
import requests
import json
import time
from typing import Dict, Any, List, Optional, Union
from urllib.parse import urljoin, urlparse, quote
from dataclasses import dataclass, asdict
from .base_tool import Tool, ToolResult

try:
    from bs4 import BeautifulSoup
    BEAUTIFULSOUP_AVAILABLE = True
except ImportError:
    BEAUTIFULSOUP_AVAILABLE = False


@dataclass
class SearchResult:
    """Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ¿Ğ¾Ğ¸ÑĞºĞ°"""
    title: str
    url: str
    snippet: str
    source: str
    relevance_score: float = 0.0
    timestamp: Optional[str] = None


@dataclass
class ScrapingResult:
    """Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ scraping"""
    url: str
    success: bool
    data: Dict[str, Any]
    metadata: Optional[Dict[str, Any]] = None
    error: Optional[str] = None


@dataclass
class ApiResponse:
    """Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ API Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°"""
    url: str
    status_code: int
    success: bool
    data: Dict[str, Any]
    headers: Dict[str, str]
    response_time: float
    error: Optional[str] = None


# ĞĞ±Ñ‰Ğ¸Ğµ ÑƒÑ‚Ğ¸Ğ»Ğ¸Ñ‚Ñ‹
def is_valid_url(url: str) -> bool:
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ URL"""
    try:
        result = urlparse(url)
        return all([result.scheme, result.netloc])
    except Exception:
        return False


def clean_text(text: str) -> str:
    """ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¾Ñ‚ Ğ»Ğ¸ÑˆĞ½Ğ¸Ñ… ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²"""
    if not text:
        return ""
    
    # Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ»Ğ¸ÑˆĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑ‹
    text = ' '.join(text.split())
    
    # Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ‹
    text = ''.join(char for char in text if ord(char) >= 32 or char in '\n\t')
    
    return text.strip()


def create_headers(user_agent: str = None) -> Dict[str, str]:
    """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… HTTP Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¾Ğ²"""
    if not user_agent:
        user_agent = "KittyCore-WebTools/3.0"
    
    return {
        'User-Agent': user_agent,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }


def extract_domain(url: str) -> str:
    """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ° Ğ¸Ğ· URL"""
    try:
        parsed = urlparse(url)
        return parsed.netloc
    except Exception:
        return ""


def format_file_size(size_bytes: int) -> str:
    """Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ñ„Ğ°Ğ¹Ğ»Ğ°"""
    if size_bytes == 0:
        return "0 B"
    
    size_names = ["B", "KB", "MB", "GB"]
    i = 0
    while size_bytes >= 1024 and i < len(size_names) - 1:
        size_bytes /= 1024.0
        i += 1
    
    return f"{size_bytes:.2f} {size_names[i]}"


class WebSession:
    """Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ HTTP ÑĞµÑÑĞ¸ĞµĞ¹"""
    
    def __init__(self, timeout: int = 30):
        self.timeout = timeout
        self.session = None
    
    async def __aenter__(self):
        """Async context manager entry"""
        timeout = aiohttp.ClientTimeout(total=self.timeout)
        self.session = aiohttp.ClientSession(
            timeout=timeout,
            headers=create_headers()
        )
        return self.session
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        if self.session:
            await self.session.close() 