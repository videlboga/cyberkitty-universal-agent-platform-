#!/usr/bin/env python3
"""
üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: EnhancedWebScrapingTool

–ü–†–û–ë–õ–ï–ú–ê: –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ - –Ω—É–∂–µ–Ω urls –∫–∞–∫ —Å–ø–∏—Å–æ–∫, –∞ –Ω–µ url –∫–∞–∫ —Å—Ç—Ä–æ–∫–∞
"""

import asyncio

async def test_web_scraping_fix():
    """–¢–µ—Å—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è EnhancedWebScrapingTool"""
    print("üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: EnhancedWebScrapingTool")
    print("=" * 50)
    
    from kittycore.tools.enhanced_web_scraping_tool import EnhancedWebScrapingTool
    tool = EnhancedWebScrapingTool()
    
    # –¢–µ—Å—Ç 1: –°–∫—Ä–∞–ø–∏–Ω–≥ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    print("\nüåê –¢–µ—Å—Ç 1: –°–∫—Ä–∞–ø–∏–Ω–≥ httpbin.org")
    result1 = await tool.execute(
        urls=["https://httpbin.org/html"],  # –°–ø–∏—Å–æ–∫ URL, –∞ –Ω–µ —Å—Ç—Ä–æ–∫–∞
        extract_links=True,
        extract_metadata=True,
        filter_text=True
    )
    print(f"‚úÖ –°–∫—Ä–∞–ø–∏–Ω–≥: success={result1.success}")
    if result1.data:
        print(f"üìä –î–∞–Ω–Ω—ã–µ: {len(str(result1.data))} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"üìã –†–µ–∑—É–ª—å—Ç–∞—Ç: {result1.data.get('total_urls')} URL, {result1.data.get('successful_scrapes')} —É—Å–ø–µ—à–Ω—ã—Ö")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
        if result1.data.get('results'):
            first_result = result1.data['results'][0]
            print(f"üìã –ü–µ—Ä–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {len(str(first_result))} —Å–∏–º–≤–æ–ª–æ–≤")
    if result1.error:
        print(f"‚ùå –û—à–∏–±–∫–∞: {result1.error}")
    
    # –¢–µ—Å—Ç 2: –°–∫—Ä–∞–ø–∏–Ω–≥ –ø—Ä–æ—Å—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    print("\nüìÑ –¢–µ—Å—Ç 2: –°–∫—Ä–∞–ø–∏–Ω–≥ –ø—Ä–æ—Å—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã")
    result2 = await tool.execute(
        urls=["https://httpbin.org/"],  # –ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ httpbin
        extract_links=False,
        extract_metadata=True,
        filter_text=True
    )
    print(f"‚úÖ –°–∫—Ä–∞–ø–∏–Ω–≥: success={result2.success}")
    if result2.data:
        print(f"üìä –î–∞–Ω–Ω—ã–µ: {len(str(result2.data))} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"üìã –†–µ–∑—É–ª—å—Ç–∞—Ç: {result2.data.get('total_urls')} URL, {result2.data.get('successful_scrapes')} —É—Å–ø–µ—à–Ω—ã—Ö")
    if result2.error:
        print(f"‚ùå –û—à–∏–±–∫–∞: {result2.error}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—â–∏–π —É—Å–ø–µ—Ö
    if result1.success and result2.success:
        print(f"\nüéâ EnhancedWebScrapingTool –ü–û–õ–ù–û–°–¢–¨–Æ –ò–°–ü–†–ê–í–õ–ï–ù!")
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: 2/2 —Ç–µ—Å—Ç–∞ —É—Å–ø–µ—à–Ω–æ")
        return True
    elif result1.success or result2.success:
        print(f"\n‚ö†Ô∏è EnhancedWebScrapingTool —á–∞—Å—Ç–∏—á–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç")
        successful_tests = sum([result1.success, result2.success])
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {successful_tests}/2 —Ç–µ—Å—Ç–æ–≤ —É—Å–ø–µ—à–Ω–æ")
        return True  # –°—á–∏—Ç–∞–µ–º —É—Å–ø–µ—Ö–æ–º –µ—Å–ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ç–µ—Å—Ç —Ä–∞–±–æ—Ç–∞–µ—Ç
    else:
        print(f"\n‚ùå EnhancedWebScrapingTool –≤—Å—ë –µ—â—ë –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç")
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: 0/2 —Ç–µ—Å—Ç–æ–≤ —É—Å–ø–µ—à–Ω–æ")
        return False

if __name__ == "__main__":
    asyncio.run(test_web_scraping_fix()) 