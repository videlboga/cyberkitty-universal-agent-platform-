#!/usr/bin/env python3
"""
üîç –ê–ù–ê–õ–ò–ó –í–û–ó–ú–û–ñ–ù–û–°–¢–ï–ô –õ–û–ö–ê–õ–¨–ù–´–• –ú–û–î–ï–õ–ï–ô

–ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –º–æ–∂–Ω–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å –Ω–∞ RTX 3070 Ti (8GB VRAM)
"""

import subprocess
import importlib
import sys

print("üîç –ê–ù–ê–õ–ò–ó –°–ò–°–¢–ï–ú–´ –î–õ–Ø –õ–û–ö–ê–õ–¨–ù–´–• LLM")
print("="*60)

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
libraries_to_check = [
    "transformers",
    "torch", 
    "accelerate",
    "bitsandbytes",
    "sentencepiece",
    "protobuf",
    "numpy",
    "huggingface_hub"
]

print("\nüì¶ –ü–†–û–í–ï–†–ö–ê –ë–ò–ë–õ–ò–û–¢–ï–ö:")
available_libs = {}
for lib in libraries_to_check:
    try:
        module = importlib.import_module(lib)
        version = getattr(module, '__version__', 'unknown')
        available_libs[lib] = version
        print(f"   ‚úÖ {lib}: {version}")
    except ImportError:
        available_libs[lib] = None
        print(f"   ‚ùå {lib}: –ù–ï –£–°–¢–ê–ù–û–í–õ–ï–ù")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º GPU
print(f"\nüñ•Ô∏è –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –°–ò–°–¢–ï–ú–ï:")
try:
    import torch
    print(f"   üî• CUDA: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"   üéÆ GPU: {torch.cuda.get_device_name(0)}")
        print(f"   üíæ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        print(f"   ‚ö° CUDA –≤–µ—Ä—Å–∏—è: {torch.version.cuda}")
except:
    print("   ‚ùå PyTorch –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω")

# –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è RTX 3070 Ti (8GB)
print(f"\nü§ñ –†–ï–ö–û–ú–ï–ù–î–£–ï–ú–´–ï –õ–û–ö–ê–õ–¨–ù–´–ï –ú–û–î–ï–õ–ò (8GB VRAM):")

models_8gb = [
    {
        "name": "microsoft/DialoGPT-medium",
        "size": "1.4GB",
        "speed": "–û—á–µ–Ω—å –±—ã—Å—Ç—Ä–∞—è",
        "quality": "–•–æ—Ä–æ—à–∞—è –¥–ª—è –¥–∏–∞–ª–æ–≥–æ–≤",
        "setup": "pip install transformers torch",
        "good_for": "–ë—ã—Å—Ç—Ä—ã–µ —Ç–µ—Å—Ç—ã, –≤–∞–ª–∏–¥–∞—Ü–∏—è"
    },
    {
        "name": "microsoft/DialoGPT-large", 
        "size": "3.0GB",
        "speed": "–ë—ã—Å—Ç—Ä–∞—è",
        "quality": "–û—Ç–ª–∏—á–Ω–∞—è –¥–ª—è –¥–∏–∞–ª–æ–≥–æ–≤",
        "setup": "pip install transformers torch",
        "good_for": "–ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è"
    },
    {
        "name": "google/flan-t5-large",
        "size": "3.0GB", 
        "speed": "–ë—ã—Å—Ç—Ä–∞—è",
        "quality": "–û—Ç–ª–∏—á–Ω–∞—è –¥–ª—è –∑–∞–¥–∞—á",
        "setup": "pip install transformers torch",
        "good_for": "Instruction following, –∑–∞–¥–∞—á–∏"
    },
    {
        "name": "microsoft/CodeT5-large",
        "size": "3.0GB",
        "speed": "–ë—ã—Å—Ç—Ä–∞—è", 
        "quality": "–û—Ç–ª–∏—á–Ω–∞—è –¥–ª—è –∫–æ–¥–∞",
        "setup": "pip install transformers torch",
        "good_for": "–ö–æ–¥, –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ"
    },
    {
        "name": "mistralai/Mistral-7B-Instruct-v0.1",
        "size": "7GB",
        "speed": "–°—Ä–µ–¥–Ω—è—è",
        "quality": "–ü—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω–∞—è",
        "setup": "pip install transformers torch bitsandbytes",
        "good_for": "–ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π LLM, –ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ"
    }
]

for i, model in enumerate(models_8gb, 1):
    print(f"\n{i}. ü§ñ {model['name']}")
    print(f"   üì¶ –†–∞–∑–º–µ—Ä: {model['size']}")
    print(f"   ‚ö° –°–∫–æ—Ä–æ—Å—Ç—å: {model['speed']}")
    print(f"   üéØ –ö–∞—á–µ—Å—Ç–≤–æ: {model['quality']}")
    print(f"   üîß –£—Å—Ç–∞–Ω–æ–≤–∫–∞: {model['setup']}")
    print(f"   ‚ú® –õ—É—á—à–µ –¥–ª—è: {model['good_for']}")

# Ollama –≤–∞—Ä–∏–∞–Ω—Ç
print(f"\nü¶ô –ê–õ–¨–¢–ï–†–ù–ê–¢–ò–í–ê - OLLAMA:")
print(f"   üì• –£—Å—Ç–∞–Ω–æ–≤–∫–∞: curl -fsSL https://ollama.ai/install.sh | sh")
print(f"   ü§ñ –ú–æ–¥–µ–ª–∏ –¥–ª—è 8GB:")
print(f"      ‚Ä¢ llama3.2:3b (3GB) - –±—ã—Å—Ç—Ä–∞—è, —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è")
print(f"      ‚Ä¢ mistral:7b (7GB) - –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è")
print(f"      ‚Ä¢ codellama:7b (7GB) - –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è")
print(f"   ‚ö° –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞: –∞–≤—Ç–æ—É—Å—Ç–∞–Ω–æ–≤–∫–∞, –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è, –ø—Ä–æ—Å—Ç–æ—Ç–∞")

# –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
print(f"\nüéØ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –î–õ–Ø KITTYCORE:")

if available_libs["transformers"] and available_libs["torch"]:
    print(f"   ‚úÖ –ì–û–¢–û–í–û –ö –†–ê–ë–û–¢–ï:")
    print(f"      1. –ë—ã—Å—Ç—Ä—ã–µ —Ç–µ—Å—Ç—ã: DialoGPT-medium (1.4GB)")
    print(f"      2. –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è: FLAN-T5-large (3GB)") 
    print(f"      3. –ü–æ–ª–Ω—ã–π LLM: Mistral-7B (7GB)")
else:
    print(f"   ‚ö†Ô∏è –ù–£–ñ–ù–ê –£–°–¢–ê–ù–û–í–ö–ê:")
    missing = [lib for lib, ver in available_libs.items() if ver is None and lib in ["transformers", "torch"]]
    print(f"      pip install {' '.join(missing)}")

print(f"\nüöÄ –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò:")
print(f"   1. –í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –∏–∑ —Å–ø–∏—Å–∫–∞ –≤—ã—à–µ")
print(f"   2. –Ø —Å–æ–∑–¥–∞–º –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é –≤ KittyCore")
print(f"   3. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é —Ä–∞–±–æ—Ç—É")
print(f"   4. –ù–∞—Å—Ç—Ä–æ–∏–º fallback –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏")

if __name__ == "__main__":
    print(f"\nüîç –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω. –ö–∞–∫—É—é –º–æ–¥–µ–ª—å –≤—ã–±–µ—Ä–µ—Ç–µ?") 