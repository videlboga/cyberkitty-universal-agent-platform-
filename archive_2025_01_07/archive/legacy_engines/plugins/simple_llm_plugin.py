"""
Simple LLM Plugin - –£–ø—Ä–æ—â—ë–Ω–Ω—ã–π –ø–ª–∞–≥–∏–Ω –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏

–ü—Ä–∏–Ω—Ü–∏–ø: –ü–†–û–°–¢–û–¢–ê –ü–†–ï–í–´–®–ï –í–°–ï–ì–û!
- –¢–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏: –æ—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –ø–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤
- –ú–∏–Ω–∏–º—É–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
- –ü—Ä–æ—Å—Ç–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
- –í–æ–∑–≤—Ä–∞—Ç —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
"""

import os
import json
import httpx
from datetime import datetime
from typing import Dict, Any, Optional
from loguru import logger

from app.core.base_plugin import BasePlugin


class SimpleLLMPlugin(BasePlugin):
    """–ü—Ä–æ—Å—Ç–æ–π –ø–ª–∞–≥–∏–Ω –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LLM —á–µ—Ä–µ–∑ OpenRouter API"""
    
    def __init__(self):
        super().__init__("simple_llm")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ LLM (–±—É–¥—É—Ç –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –ë–î)
        self.api_key = None
        self.openai_api_key = None
        self.anthropic_api_key = None
        self.base_url = "https://openrouter.ai/api/v1/chat/completions"
        self.default_model = "meta-llama/llama-3.2-3b-instruct:free"
        
        logger.info("SimpleLLMPlugin –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    async def _do_initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–ª–∞–≥–∏–Ω–∞"""
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –ë–î
        await self._load_settings_from_db()
            
        if self.api_key or self.openai_api_key or self.anthropic_api_key:
            logger.info(f"‚úÖ SimpleLLMPlugin –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ —Å –º–æ–¥–µ–ª—å—é: {self.default_model}")
        else:
            logger.warning("‚ö†Ô∏è SimpleLLMPlugin —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–º —Ä–µ–∂–∏–º–µ - API –∫–ª—é—á–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –ë–î")
            logger.info("üí° –î–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–æ–±–∞–≤—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é settings —Å plugin_name: 'llm'")
    
    async def _load_settings_from_db(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ LLM –∏–∑ MongoDB"""
        try:
            if not self.engine or not hasattr(self.engine, 'plugins') or 'mongo' not in self.engine.plugins:
                logger.warning("MongoDB –ø–ª–∞–≥–∏–Ω –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–∞—Å—Ç—Ä–æ–µ–∫ LLM")
                return
                
            mongo_plugin = self.engine.plugins['mongo']
            
            # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ —Ä–∞–∑–Ω—ã–º –≤–∞—Ä–∏–∞–Ω—Ç–∞–º plugin_name
            settings_result = None
            
            # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –ø–æ "llm"
            settings_result = await mongo_plugin._find_one("settings", {"plugin_name": "llm"})
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –∏—â–µ–º –ø–æ "simple_llm"
            if not (settings_result and settings_result.get("success") and settings_result.get("document")):
                settings_result = await mongo_plugin._find_one("settings", {"plugin_name": "simple_llm"})
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –∏—â–µ–º –ø–æ –ø–æ–ª—é "plugin"
            if not (settings_result and settings_result.get("success") and settings_result.get("document")):
                settings_result = await mongo_plugin._find_one("settings", {"plugin": "simple_llm"})
            
            logger.info(f"üîç –û–¢–õ–ê–î–ö–ê settings_result: {settings_result}")
            
            if settings_result and settings_result.get("success") and settings_result.get("document"):
                settings = settings_result["document"]
                logger.info(f"üîç –û–¢–õ–ê–î–ö–ê document: {settings}")
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º API –∫–ª—é—á–∏ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –ø–æ–ª–µ–π
                self.api_key = (settings.get("openrouter_api_key") or 
                               settings.get("api_key") or 
                               settings.get("openai_api_key"))  # –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                               
                self.openai_api_key = settings.get("openai_api_key")
                self.anthropic_api_key = settings.get("anthropic_api_key")
                self.default_model = settings.get("default_model", self.default_model)
                
                if self.api_key:
                    logger.info("‚úÖ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ LLM –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –ë–î - API –∫–ª—é—á –Ω–∞–π–¥–µ–Ω")
                else:
                    logger.warning("‚ö†Ô∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ LLM –Ω–∞–π–¥–µ–Ω—ã –≤ –ë–î, –Ω–æ API –∫–ª—é—á –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
            else:
                logger.info("‚ö†Ô∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ LLM –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –ë–î")
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–∞—Å—Ç—Ä–æ–µ–∫ LLM –∏–∑ –ë–î: {e}")

    async def _ensure_fresh_settings(self):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –ë–î –ø–µ—Ä–µ–¥ –∫–∞–∂–¥—ã–º –∑–∞–ø—Ä–æ—Å–æ–º"""
        await self._load_settings_from_db()
    
    # === –ú–ï–¢–û–î–´ –î–õ–Ø –ù–ê–°–¢–†–û–ô–ö–ò –ß–ï–†–ï–ó API ===
    
    async def save_settings_to_db(self, openrouter_api_key: str = None, openai_api_key: str = None, 
                                 anthropic_api_key: str = None, default_model: str = None) -> Dict[str, Any]:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ LLM –≤ MongoDB (–¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ API)"""
        try:
            if not self.engine or not hasattr(self.engine, 'plugins') or 'mongo' not in self.engine.plugins:
                return {"success": False, "error": "MongoDB –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"}
                
            mongo_plugin = self.engine.plugins['mongo']
            
            settings_doc = {
                "plugin_name": "llm",
                "plugin": "simple_llm",  # –î–æ–±–∞–≤–ª—è–µ–º –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                "openrouter_api_key": openrouter_api_key,
                "api_key": openrouter_api_key,  # –î—É–±–ª–∏—Ä—É–µ–º –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏  
                "openai_api_key": openai_api_key,
                "anthropic_api_key": anthropic_api_key,
                "default_model": default_model or self.default_model,
                "updated_at": datetime.now().isoformat()
            }
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º upsert –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏—è –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ settings
            result = await mongo_plugin._update_one(
                "settings", 
                {"plugin_name": "llm"}, 
                {"$set": settings_doc},
                upsert=True
            )
            
            if result.get("success"):
                # –û–±–Ω–æ–≤–ª—è–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤ –ø–ª–∞–≥–∏–Ω–µ
                if openrouter_api_key:
                    self.api_key = openrouter_api_key
                if openai_api_key:
                    self.openai_api_key = openai_api_key
                if anthropic_api_key:
                    self.anthropic_api_key = anthropic_api_key
                if default_model:
                    self.default_model = default_model
                
                logger.info("‚úÖ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ LLM —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ë–î –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã")
                return {"success": True, "message": "–ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã"}
            else:
                error_msg = result.get('error', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ LLM –≤ –ë–î: {error_msg}")
                return {"success": False, "error": error_msg}
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–∫ LLM –≤ –ë–î: {e}")
            return {"success": False, "error": str(e)}
    
    def get_current_settings(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–ª–∞–≥–∏–Ω–∞"""
        return {
            "openrouter_api_key": "***" if self.api_key else None,
            "openai_api_key": "***" if self.openai_api_key else None,
            "anthropic_api_key": "***" if self.anthropic_api_key else None,
            "default_model": self.default_model,
            "any_key_set": bool(self.api_key or self.openai_api_key or self.anthropic_api_key),
            "configured": bool(self.api_key or self.openai_api_key or self.anthropic_api_key)
        }

    def register_handlers(self) -> Dict[str, Any]:
        """–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ —à–∞–≥–æ–≤"""
        return {
            "llm_query": self._handle_llm_query,
            "llm_chat": self._handle_llm_chat,
            "build_prompt": self._handle_build_prompt  # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä –ø—Ä–æ–º–ø—Ç–æ–≤
        }
    
    async def _handle_llm_query(self, step_data: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """
        –û–±—Ä–∞–±–æ—Ç—á–∏–∫ —à–∞–≥–∞ llm_query - –ø—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å –∫ LLM
        
        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        - prompt: —Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)
        - model: –º–æ–¥–µ–ª—å (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ env)
        - system_prompt: —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        - temperature: —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.7)
        - max_tokens: –º–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 500)
        - output_var: –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é "llm_response")
        """
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–≤–µ–∂–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –ë–î
        await self._ensure_fresh_settings()
        
        params = step_data.get("params", {})
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–æ–π –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        prompt = self._resolve_value(params.get("prompt", ""), context)
        model = params.get("model", self.default_model)
        system_prompt = self._resolve_value(params.get("system_prompt", ""), context)
        temperature = params.get("temperature", 0.7)
        max_tokens = params.get("max_tokens", 500)
        output_var = params.get("output_var", "llm_response")
        
        if not prompt:
            logger.error("SimpleLLMPlugin: prompt –Ω–µ —É–∫–∞–∑–∞–Ω")
            context[output_var] = {"error": "Prompt –Ω–µ —É–∫–∞–∑–∞–Ω"}
            return context
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å
        response = await self._make_llm_request(
            prompt=prompt,
            model=model,
            system_prompt=system_prompt,
            temperature=temperature,
            max_tokens=max_tokens
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—É—é —á–∞—Å—Ç—å –æ—Ç–≤–µ—Ç–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç
        if response.get("success"):
            context[output_var] = response["content"]
            logger.info(f"LLM –æ—Ç–≤–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {output_var}: {response['content'][:100]}...")
        else:
            context[output_var] = {"error": response.get("error", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞")}
            logger.error(f"–û—à–∏–±–∫–∞ LLM: {response.get('error')}")
        
        return context
    
    async def _handle_llm_chat(self, step_data: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """
        –û–±—Ä–∞–±–æ—Ç—á–∏–∫ —à–∞–≥–∞ llm_chat - —á–∞—Ç —Å –∏—Å—Ç–æ—Ä–∏–µ–π —Å–æ–æ–±—â–µ–Ω–∏–π
        
        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        - messages: —Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π [{"role": "user/assistant", "content": "..."}]
        - model: –º–æ–¥–µ–ª—å (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        - temperature: —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        - max_tokens: –º–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        - output_var: –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é "llm_response")
        """
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–≤–µ–∂–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –ë–î
        await self._ensure_fresh_settings()
        
        params = step_data.get("params", {})
        
        messages = params.get("messages", [])
        model = params.get("model", self.default_model)
        temperature = params.get("temperature", 0.7)
        max_tokens = params.get("max_tokens", 500)
        output_var = params.get("output_var", "llm_response")
        
        if not messages:
            logger.error("SimpleLLMPlugin: messages –Ω–µ —É–∫–∞–∑–∞–Ω—ã")
            context[output_var] = {"error": "Messages –Ω–µ —É–∫–∞–∑–∞–Ω—ã"}
            return context
        
        # –†–∞–∑—Ä–µ—à–∞–µ–º –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä—ã –≤ —Å–æ–æ–±—â–µ–Ω–∏—è—Ö
        resolved_messages = []
        for msg in messages:
            resolved_msg = {
                "role": msg.get("role", "user"),
                "content": self._resolve_value(msg.get("content", ""), context)
            }
            resolved_messages.append(resolved_msg)
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å
        response = await self._make_llm_request(
            messages=resolved_messages,
            model=model,
            temperature=temperature,
            max_tokens=max_tokens
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–≤–µ—Ç
        if response.get("success"):
            context[output_var] = response["content"]
            logger.info(f"LLM —á–∞—Ç –æ—Ç–≤–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {output_var}")
        else:
            context[output_var] = {"error": response.get("error", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞")}
            logger.error(f"–û—à–∏–±–∫–∞ LLM —á–∞—Ç–∞: {response.get('error')}")
        
        return context
    
    async def _handle_build_prompt(self, step_data: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """
        –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è –∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤ –∏–∑ —à–∞–±–ª–æ–Ω–æ–≤ –∏ –¥–∞–Ω–Ω—ã—Ö.
        
        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        - template: —à–∞–±–ª–æ–Ω –ø—Ä–æ–º–ø—Ç–∞ —Å –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–∞–º–∏ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)
        - variables: —Å–ª–æ–≤–∞—Ä—å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        - format_type: —Ç–∏–ø —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è ("simple", "json", "list") (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        - output_var: –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é "built_prompt")
        
        –ü—Ä–∏–º–µ—Ä:
        {
            "type": "build_prompt",
            "params": {
                "template": "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–∞–Ω–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:\n–ò–º—è: {name}\n–û—Ç–≤–µ—Ç—ã: {answers}",
                "variables": {
                    "name": "{user_profile.name}",
                    "answers": "{diagnosis_answers}"
                },
                "format_type": "simple",
                "output_var": "analysis_prompt"
            }
        }
        """
        params = step_data.get("params", {})
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        template = params.get("template", "")
        variables = params.get("variables", {})
        format_type = params.get("format_type", "simple")
        output_var = params.get("output_var", "built_prompt")
        
        if not template:
            logger.error("build_prompt: template –Ω–µ —É–∫–∞–∑–∞–Ω")
            context[output_var] = {"error": "Template –Ω–µ —É–∫–∞–∑–∞–Ω"}
            return context
        
        try:
            # –ü–æ–¥—Å—Ç–∞–≤–ª—è–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            resolved_variables = {}
            for key, value in variables.items():
                resolved_variables[key] = self._resolve_value(value, context)
            
            # –ö–æ–Ω—Å—Ç—Ä—É–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
            if format_type == "json":
                # JSON —Ñ–æ—Ä–º–∞—Ç —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –∫–∞–≤—ã—á–∫–∞–º–∏
                built_prompt = template.format(**{
                    k: json.dumps(v, ensure_ascii=False) if isinstance(v, (dict, list)) else str(v)
                    for k, v in resolved_variables.items()
                })
            elif format_type == "list":
                # –°–ø–∏—Å–æ–∫ —Ñ–æ—Ä–º–∞—Ç
                built_prompt = template.format(**{
                    k: '\n'.join([f"- {item}" for item in v]) if isinstance(v, list) else str(v)
                    for k, v in resolved_variables.items()
                })
            else:  # simple
                # –ü—Ä–æ—Å—Ç–∞—è –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å—Ç—Ä–æ–∫
                built_prompt = template.format(**{
                    k: str(v) for k, v in resolved_variables.items()
                })
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            context[output_var] = {
                "success": True,
                "template": template,
                "variables": resolved_variables,
                "built_prompt": built_prompt,
                "format_type": format_type
            }
            
            logger.info(f"‚úÖ –ü—Ä–æ–º–ø—Ç –ø–æ—Å—Ç—Ä–æ–µ–Ω —É—Å–ø–µ—à–Ω–æ: {len(built_prompt)} —Å–∏–º–≤–æ–ª–æ–≤")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–º–ø—Ç–∞: {e}")
            context[output_var] = {
                "success": False,
                "error": str(e),
                "template": template,
                "variables": variables
            }
        
        return context
    
    async def _make_llm_request(self, prompt: str = None, messages: list = None, 
                               model: str = None, system_prompt: str = None,
                               temperature: float = 0.7, max_tokens: int = 500) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ HTTP –∑–∞–ø—Ä–æ—Å–∞ –∫ LLM API"""
        
        # –í—ã–±–∏—Ä–∞–µ–º –ø–æ–¥—Ö–æ–¥—è—â–∏–π API –∫–ª—é—á
        api_key = self.api_key or self.openai_api_key or self.anthropic_api_key
        
        logger.info(f"üîç –û–¢–õ–ê–î–ö–ê API –∫–ª—é—á–µ–π: api_key={'***' if self.api_key else None}, openai_api_key={'***' if self.openai_api_key else None}, anthropic_api_key={'***' if self.anthropic_api_key else None}")
        logger.info(f"üîç –û–¢–õ–ê–î–ö–ê –≤—ã–±—Ä–∞–Ω–Ω—ã–π api_key: {'***' + api_key[-10:] if api_key and len(api_key) > 10 else api_key}")
        
        if not api_key:
            return {"success": False, "error": "API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω"}
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
        if messages:
            request_messages = messages
        elif prompt:
            request_messages = []
            if system_prompt:
                request_messages.append({"role": "system", "content": system_prompt})
            request_messages.append({"role": "user", "content": prompt})
        else:
            return {"success": False, "error": "–ù–µ —É–∫–∞–∑–∞–Ω prompt –∏–ª–∏ messages"}
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º payload
        payload = {
            "model": model or self.default_model,
            "messages": request_messages,
            "temperature": temperature,
            "max_tokens": max_tokens
        }
        
        logger.info(f"üîç –û–¢–õ–ê–î–ö–ê payload: model={payload['model']}, messages_count={len(request_messages)}")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º HTTP –∑–∞–ø—Ä–æ—Å
        try:
            async with httpx.AsyncClient() as client:
                headers = {
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json",
                    "HTTP-Referer": "https://universal-agent-platform.local",
                    "X-Title": "Universal Agent Platform"
                }
                
                logger.info(f"üîç –û–¢–õ–ê–î–ö–ê headers Authorization: {'Bearer ' + api_key[:20] + '***' if api_key else 'None'}")
                
                response = await client.post(
                    self.base_url,
                    json=payload,
                    headers=headers,
                    timeout=30.0
                )
                
                logger.info(f"üîç –û–¢–õ–ê–î–ö–ê response status: {response.status_code}")
                
                if response.status_code == 200:
                    data = response.json()
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞
                    if data.get("choices") and len(data["choices"]) > 0:
                        content = data["choices"][0]["message"]["content"]
                        return {"success": True, "content": content}
                    else:
                        return {"success": False, "error": "–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç API"}
                else:
                    error_text = response.text
                    logger.error(f"LLM API –æ—à–∏–±–∫–∞ {response.status_code}: {error_text}")
                    return {"success": False, "error": f"API –æ—à–∏–±–∫–∞ {response.status_code}"}
                    
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ LLM: {e}")
            return {"success": False, "error": str(e)}
    
    def _resolve_value(self, value: Any, context: Dict[str, Any]) -> Any:
        """–ü—Ä–æ—Å—Ç–∞—è –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–Ω–∞—á–µ–Ω–∏–π –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        if isinstance(value, str) and "{" in value and "}" in value:
            try:
                return value.format(**context)
            except (KeyError, ValueError) as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑—Ä–µ—à–∏—Ç—å '{value}': {e}")
                return value
        return value
    
    async def healthcheck(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è –ø–ª–∞–≥–∏–Ω–∞"""
        if not self.api_key:
            return False
        
        try:
            # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            response = await self._make_llm_request(
                prompt="test",
                max_tokens=1
            )
            return response.get("success", False)
        except Exception:
            return False 