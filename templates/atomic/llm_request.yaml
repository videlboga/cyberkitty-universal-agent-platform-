# üß© –ê–¢–û–ú–ê–†–ù–´–ô –®–ê–ë–õ–û–ù: LLM –ó–ê–ü–†–û–°
# –ù–∞–¥–µ–∂–Ω—ã–π –±–ª–æ–∫ –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏

atomic_template:
  type: "llm_request"
  name: "LLM –∑–∞–ø—Ä–æ—Å"
  version: "1.0.0"
  description: "–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç–≤–µ—Ç"
  
  # –°—Ö–µ–º–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
  config_schema:
    prompt:                 # string, required - –ü—Ä–æ–º–ø—Ç –¥–ª—è LLM
      type: "string"
      required: true
      description: "–¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ –∫ LLM, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —à–∞–±–ª–æ–Ω—ã {var}"
      
    model:                  # string, optional - –ú–æ–¥–µ–ª—å LLM
      type: "string"
      default: "deepseek/deepseek-chat"
      description: "–ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ LLM"
      
    max_tokens:             # integer, optional - –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤
      type: "integer"
      default: 1000
      description: "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ"
      
    temperature:            # float, optional - –ö—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
      type: "float"
      default: 0.7
      description: "–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏ (0.0-1.0)"
      
    system_prompt:          # string, optional - –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
      type: "string"
      default: ""
      description: "–°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –º–æ–¥–µ–ª–∏"
      
    response_format:        # string, optional - –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞
      type: "string"
      default: "text"
      enum: ["text", "json"]
      description: "–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM"
      
    save_to_context:        # string, optional - –ö–ª—é—á –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
      type: "string"
      default: "llm_response"
      description: "–ö–ª—é—á –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞"

  # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
  example:
    type: "llm_request"
    config:
      prompt: "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–æ—Å–∏–ª: {user_question}. –û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É."
      model: "deepseek/deepseek-chat"
      max_tokens: 500
      temperature: 0.7
      system_prompt: "–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."
      save_to_context: "assistant_reply"

  # –ü–æ–≤–µ–¥–µ–Ω–∏–µ
  behavior:
    generates_buttons: false          # –ù–µ —Å–æ–∑–¥–∞–µ—Ç –∫–Ω–æ–ø–∫–∏
    expected_event_type: "any"        # –ü—Ä–∏–Ω–∏–º–∞–µ—Ç –ª—é–±–æ–µ —Å–æ–±—ã—Ç–∏–µ
    response_type: "immediate"        # –û—Ç–≤–µ—á–∞–µ—Ç –º–≥–Ω–æ–≤–µ–Ω–Ω–æ
    state_change: "always"            # –í—Å–µ–≥–¥–∞ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç –∫ next_step
    external_api: true                # –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–Ω–µ—à–Ω–∏–π API
    
  # –ö–æ–Ω—Ç–µ–∫—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–∑–¥–∞–µ—Ç—Å—è
  context_updates:
    "{save_to_context}": "–û—Ç–≤–µ—Ç –æ—Ç LLM"
    "last_llm_model": "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å"
    "last_llm_tokens": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç—Ä–∞—á–µ–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤"
    
  # –í–æ–∑–º–æ–∂–Ω—ã–µ –æ—à–∏–±–∫–∏
  error_cases:
    - api_error: "–û—à–∏–±–∫–∞ API LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞"
    - timeout: "–ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞"
    - invalid_model: "–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–∞—è –º–æ–¥–µ–ª—å"
    - token_limit: "–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤"

# –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –ø—Ä–æ—Å—Ç–æ–π state machine
simple_implementation: |
  class LLMStep(BaseStep):
      async def execute(self, event: Event, state: UserState) -> StepResult:
          try:
              # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–æ–º–ø—Ç
              prompt = self.resolve_template(self.config["prompt"], state.context)
              
              # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è LLM
              llm_params = {
                  "model": self.config.get("model", "deepseek/deepseek-chat"),
                  "max_tokens": self.config.get("max_tokens", 1000),
                  "temperature": self.config.get("temperature", 0.7)
              }
              
              # –í—ã–∑—ã–≤–∞–µ–º LLM (–∑–¥–µ—Å—å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ä–µ–∞–ª—å–Ω—ã–π API –≤—ã–∑–æ–≤)
              llm_response = await self.call_llm_api(prompt, llm_params)
              
              # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç
              save_key = self.config.get("save_to_context", "llm_response")
              context_updates = {
                  save_key: llm_response,
                  "last_llm_model": llm_params["model"],
                  "last_llm_tokens": len(llm_response.split())  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
              }
              
              return StepResult(
                  response=f"ü§ñ {llm_response}",
                  next_step=self.config.get("next_step"),
                  update_context=context_updates
              )
              
          except Exception as e:
              logger.error(f"LLM –æ—à–∏–±–∫–∞: {e}")
              return StepResult(
                  response="‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ LLM. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.",
                  next_step=self.config.get("error_step", self.config.get("next_step"))
              ) 