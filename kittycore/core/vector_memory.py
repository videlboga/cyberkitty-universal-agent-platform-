"""
üîç VectorMemoryStore - –í–µ–∫—Ç–æ—Ä–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è KittyCore 3.0

–ü—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤:
- ‚úÖ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –∑–∞–¥–∞—á–∞–º –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º
- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
- ‚úÖ –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Å–ø–µ—à–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π
- ‚úÖ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å–∏—Å—Ç–µ–º–æ–π –æ–±—É—á–µ–Ω–∏—è
- ‚úÖ Obsidian-—Å–æ–≤–º–µ—Å—Ç–∏–º–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ
- ‚úÖ –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–¥–∞—á

–ü—Ä–µ–≤–æ—Å—Ö–æ–¥–∏–º CrewAI, LangGraph, AutoGen –ø–æ —Å–∫–æ—Ä–æ—Å—Ç–∏ –ø–æ–∏—Å–∫–∞! üöÄ
"""

import asyncio
import json
import pickle
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Union, Tuple
from pathlib import Path
import numpy as np
from collections import defaultdict

from loguru import logger
from .obsidian_db import ObsidianDB, ObsidianNote


@dataclass
class VectorEntry:
    """–ó–∞–ø–∏—Å—å –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ø–∞–º—è—Ç–∏"""
    id: str
    content: str
    vector: List[float]
    metadata: Dict[str, Any]
    timestamp: datetime
    access_count: int = 0
    success_score: float = 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        data = asdict(self)
        data['timestamp'] = self.timestamp.isoformat()
        return data
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'VectorEntry':
        data['timestamp'] = datetime.fromisoformat(data['timestamp'])
        return cls(**data)


@dataclass
class SearchResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ø–∞–º—è—Ç–∏"""
    entry: VectorEntry
    similarity: float
    relevance_reason: str


class SimpleEmbedding:
    """–ü—Ä–æ—Å—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
    
    def __init__(self, dimension: int = 384):
        self.dimension = dimension
        # –ü—Ä–æ—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        self.vocab = {}
        self.vocab_size = 0
    
    def encode(self, text: str) -> List[float]:
        """–°–æ–∑–¥–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ö—ç—à–µ–π —Å–ª–æ–≤
        words = text.lower().split()
        vector = np.zeros(self.dimension)
        
        for word in words:
            if word not in self.vocab:
                self.vocab[word] = self.vocab_size
                self.vocab_size += 1
            
            # –ü—Ä–æ—Å—Ç–æ–µ —Ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –≤–µ–∫—Ç–æ—Ä
            word_hash = hash(word) % self.dimension
            vector[word_hash] += 1.0
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        norm = np.linalg.norm(vector)
        if norm > 0:
            vector = vector / norm
        
        return vector.tolist()
    
    def similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """–í—ã—á–∏—Å–ª–∏—Ç—å –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ"""
        v1 = np.array(vec1)
        v2 = np.array(vec2)
        
        dot_product = np.dot(v1, v2)
        norm1 = np.linalg.norm(v1)
        norm2 = np.linalg.norm(v2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return float(dot_product / (norm1 * norm2))


class VectorMemoryStore:
    """
    üîç –í–µ–∫—Ç–æ—Ä–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
    
    –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
    - –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –∑–∞–¥–∞—á –∏ —Ä–µ—à–µ–Ω–∏–π
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Å–ø–µ—à–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
    - –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å–∏—Å—Ç–µ–º–æ–π –æ–±—É—á–µ–Ω–∏—è
    - –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–¥–∞—á
    """
    
    def __init__(self, storage_path: str, obsidian_db: ObsidianDB = None):
        self.storage_path = Path(storage_path)
        self.storage_path.mkdir(parents=True, exist_ok=True)
        
        self.obsidian_db = obsidian_db
        self.embedding_model = SimpleEmbedding()
        
        # –•—Ä–∞–Ω–∏–ª–∏—â–µ –≤–µ–∫—Ç–æ—Ä–æ–≤
        self.vectors: Dict[str, VectorEntry] = {}
        self.index_file = self.storage_path / "vector_index.pkl"
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å
        self._load_index()
        
        logger.info(f"üîç VectorMemoryStore –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {storage_path}")
        logger.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(self.vectors)}")
    
    def _load_index(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∏–Ω–¥–µ–∫—Å –∏–∑ —Ñ–∞–π–ª–∞"""
        if self.index_file.exists():
            try:
                with open(self.index_file, 'rb') as f:
                    data = pickle.load(f)
                    self.vectors = {k: VectorEntry.from_dict(v) for k, v in data.items()}
                logger.info(f"üîç –ó–∞–≥—Ä—É–∂–µ–Ω –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å: {len(self.vectors)} –∑–∞–ø–∏—Å–µ–π")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–µ–∫—Å–∞: {e}")
                self.vectors = {}
    
    def _save_index(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–Ω–¥–µ–∫—Å –≤ —Ñ–∞–π–ª"""
        try:
            data = {k: v.to_dict() for k, v in self.vectors.items()}
            with open(self.index_file, 'wb') as f:
                pickle.dump(data, f)
            logger.debug(f"üíæ –í–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {len(self.vectors)} –∑–∞–ø–∏—Å–µ–π")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞: {e}")
    
    def add_task_solution(self, task_id: str, task_description: str, solution: str, 
                         success_score: float, metadata: Dict[str, Any] = None) -> str:
        """–î–æ–±–∞–≤–∏—Ç—å —Ä–µ—à–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ø–∞–º—è—Ç—å"""
        
        # –°–æ–∑–¥–∞—ë–º –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞
        content = f"–ó–∞–¥–∞—á–∞: {task_description}\n–†–µ—à–µ–Ω–∏–µ: {solution}"
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
        vector = self.embedding_model.encode(content)
        
        # –°–æ–∑–¥–∞—ë–º –∑–∞–ø–∏—Å—å
        entry = VectorEntry(
            id=task_id,
            content=content,
            vector=vector,
            metadata=metadata or {},
            timestamp=datetime.now(),
            success_score=success_score
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        self.vectors[task_id] = entry
        self._save_index()
        
        # –°–æ–∑–¥–∞—ë–º –∑–∞–º–µ—Ç–∫—É –≤ Obsidian
        if self.obsidian_db:
            self._create_solution_note(entry, task_description, solution)
        
        logger.info(f"üîç –î–æ–±–∞–≤–ª–µ–Ω–æ —Ä–µ—à–µ–Ω–∏–µ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ø–∞–º—è—Ç—å: {task_id} (—É—Å–ø–µ—Ö: {success_score:.2f})")
        
        return task_id
    
    def search_similar_tasks(self, query: str, limit: int = 5, 
                           min_similarity: float = 0.3) -> List[SearchResult]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–¥–∞—á"""
        
        if not self.vectors:
            return []
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
        query_vector = self.embedding_model.encode(query)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ —Å–æ –≤—Å–µ–º–∏ –∑–∞–ø–∏—Å—è–º–∏
        similarities = []
        for entry_id, entry in self.vectors.items():
            similarity = self.embedding_model.similarity(query_vector, entry.vector)
            
            if similarity >= min_similarity:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏—á–∏–Ω—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                reason = self._determine_relevance_reason(query, entry, similarity)
                
                similarities.append(SearchResult(
                    entry=entry,
                    similarity=similarity,
                    relevance_reason=reason
                ))
                
                # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á—ë—Ç—á–∏–∫ –¥–æ—Å—Ç—É–ø–∞
                entry.access_count += 1
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å—Ö–æ–¥—Å—Ç–≤—É –∏ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
        similarities.sort(key=lambda x: (x.similarity * 0.7 + x.entry.success_score * 0.3), reverse=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ —Å—á—ë—Ç—á–∏–∫–∏
        if similarities:
            self._save_index()
        
        logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–¥–∞—á: {len(similarities[:limit])}")
        
        return similarities[:limit]
    
    def get_successful_patterns(self, min_success_score: float = 0.7) -> List[VectorEntry]:
        """–ü–æ–ª—É—á–∏—Ç—å —É—Å–ø–µ—à–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ä–µ—à–µ–Ω–∏–π"""
        successful = [
            entry for entry in self.vectors.values()
            if entry.success_score >= min_success_score
        ]
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –∏ —á–∞—Å—Ç–æ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        successful.sort(key=lambda x: (x.success_score * 0.6 + x.access_count * 0.4), reverse=True)
        
        logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ —É—Å–ø–µ—à–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {len(successful)}")
        
        return successful
    
    def get_learning_insights(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Å–∞–π—Ç—ã –¥–ª—è —Å–∏—Å—Ç–µ–º—ã –æ–±—É—á–µ–Ω–∏—è"""
        if not self.vectors:
            return {'error': '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ø–∞–º—è—Ç–∏'}
        
        entries = list(self.vectors.values())
        
        # –ê–Ω–∞–ª–∏–∑ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
        success_scores = [e.success_score for e in entries]
        avg_success = sum(success_scores) / len(success_scores)
        
        # –ù–∞–∏–±–æ–ª–µ–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ä–µ—à–µ–Ω–∏—è
        most_accessed = sorted(entries, key=lambda x: x.access_count, reverse=True)[:5]
        
        # –ù–∞–∏–±–æ–ª–µ–µ —É—Å–ø–µ—à–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è
        most_successful = sorted(entries, key=lambda x: x.success_score, reverse=True)[:5]
        
        # –ê–Ω–∞–ª–∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        task_types = defaultdict(int)
        for entry in entries:
            task_type = entry.metadata.get('task_type', 'unknown')
            task_types[task_type] += 1
        
        insights = {
            'total_solutions': len(entries),
            'average_success_score': avg_success,
            'success_distribution': {
                'high_success': len([e for e in entries if e.success_score >= 0.8]),
                'medium_success': len([e for e in entries if 0.5 <= e.success_score < 0.8]),
                'low_success': len([e for e in entries if e.success_score < 0.5])
            },
            'most_accessed_solutions': [
                {
                    'id': e.id,
                    'access_count': e.access_count,
                    'success_score': e.success_score,
                    'task_type': e.metadata.get('task_type', 'unknown')
                }
                for e in most_accessed
            ],
            'most_successful_solutions': [
                {
                    'id': e.id,
                    'success_score': e.success_score,
                    'access_count': e.access_count,
                    'task_type': e.metadata.get('task_type', 'unknown')
                }
                for e in most_successful
            ],
            'task_type_distribution': dict(task_types),
            'generated_at': datetime.now().isoformat()
        }
        
        return insights
    
    def _determine_relevance_reason(self, query: str, entry: VectorEntry, similarity: float) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø—Ä–∏—á–∏–Ω—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏"""
        if similarity > 0.8:
            return "–û—á–µ–Ω—å –≤—ã—Å–æ–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –∑–∞–¥–∞—á"
        elif similarity > 0.6:
            return "–í—ã—Å–æ–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –∑–∞–¥–∞—á"
        elif similarity > 0.4:
            return "–£–º–µ—Ä–µ–Ω–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –∑–∞–¥–∞—á"
        else:
            return "–ù–∏–∑–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –∑–∞–¥–∞—á"
    
    def _create_solution_note(self, entry: VectorEntry, task_description: str, solution: str):
        """–°–æ–∑–¥–∞—Ç—å –∑–∞–º–µ—Ç–∫—É –æ —Ä–µ—à–µ–Ω–∏–∏ –≤ Obsidian"""
        if not self.obsidian_db:
            return
        
        content = f"""# –†–µ—à–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ {entry.id}

## –ó–∞–¥–∞—á–∞
{task_description}

## –†–µ—à–µ–Ω–∏–µ
{solution}

## –ú–µ—Ç—Ä–∏–∫–∏
- **–û—Ü–µ–Ω–∫–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏:** {entry.success_score:.2f}
- **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞—â–µ–Ω–∏–π:** {entry.access_count}
- **–î–∞—Ç–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è:** {entry.timestamp.strftime('%Y-%m-%d %H:%M:%S')}

## –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
{json.dumps(entry.metadata, indent=2, ensure_ascii=False)}

## –í–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ
- **–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å:** {len(entry.vector)}
- **–ù–æ—Ä–º–∞ –≤–µ–∫—Ç–æ—Ä–∞:** {np.linalg.norm(entry.vector):.4f}

---
*–†–µ—à–µ–Ω–∏–µ –¥–æ–±–∞–≤–ª–µ–Ω–æ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ø–∞–º—è—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏*
"""
        
        note = ObsidianNote(
            title=f"–†–µ—à–µ–Ω–∏–µ {entry.id}",
            content=content,
            tags=["–≤–µ–∫—Ç–æ—Ä–Ω–∞—è-–ø–∞–º—è—Ç—å", "—Ä–µ—à–µ–Ω–∏–µ", str(entry.metadata.get('task_type', 'general'))],
            metadata={
                "solution_id": entry.id,
                "success_score": entry.success_score,
                "task_type": entry.metadata.get('task_type', 'general'),
                "vector_dimension": len(entry.vector)
            },
            folder="system/vector_memory"
        )
        
        self.obsidian_db.save_note(note, f"solution_{entry.id}.md")
        logger.info(f"üîç –°–æ–∑–¥–∞–Ω–∞ –∑–∞–º–µ—Ç–∫–∞ –æ —Ä–µ—à–µ–Ω–∏–∏ –≤ Obsidian: {entry.id}")
    
    def cleanup_old_entries(self, days: int = 30, min_access_count: int = 1):
        """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –∑–∞–ø–∏—Å–µ–π"""
        cutoff_date = datetime.now() - timedelta(days=days)
        
        to_remove = []
        for entry_id, entry in self.vectors.items():
            if entry.timestamp < cutoff_date and entry.access_count < min_access_count:
                to_remove.append(entry_id)
        
        for entry_id in to_remove:
            del self.vectors[entry_id]
        
        if to_remove:
            self._save_index()
            logger.info(f"üîç –û—á–∏—â–µ–Ω–æ —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π: {len(to_remove)}")
        
        return len(to_remove)


def create_vector_memory_store(storage_path: str, obsidian_db: ObsidianDB = None) -> VectorMemoryStore:
    """–§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è VectorMemoryStore"""
    return VectorMemoryStore(storage_path, obsidian_db) 