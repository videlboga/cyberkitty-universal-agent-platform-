"""
üîç Vector Memory System - –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è Obsidian –±–∞–∑—ã –¥–ª—è KittyCore 3.0

–ü—Ä–µ–≤—Ä–∞—â–∞–µ–º Obsidian vault –≤ –º–æ—â–Ω—É—é –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π:
- ‚úÖ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–º–µ—Ç–æ–∫ –∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞  
- ‚úÖ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
- ‚úÖ –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è
- ‚úÖ –ü–æ–∏—Å–∫ –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –∏ —Å–º—ã—Å–ª—É

–¶–ï–õ–¨: –î–∞—Ç—å –∞–≥–µ–Ω—Ç–∞–º —Å—É–ø–µ—Ä-–ø–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º –∑–Ω–∞–Ω–∏—è–º! üß†
"""

import asyncio
import json
import logging
import hashlib
import numpy as np
from dataclasses import dataclass, field
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
import sqlite3

logger = logging.getLogger(__name__)

# === –°–¢–†–£–ö–¢–£–†–´ –î–ê–ù–ù–´–• ===

@dataclass
class VectorDocument:
    """–í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç"""
    doc_id: str
    file_path: str
    title: str
    content: str
    content_hash: str
    embedding: Optional[List[float]] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)
    chunk_index: int = 0  # –î–ª—è –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Ä–∞–∑–±–∏—Ç—ã—Ö –Ω–∞ —á–∞—Å—Ç–∏
    
    def to_dict(self) -> Dict[str, Any]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è"""
        return {
            "doc_id": self.doc_id,
            "file_path": str(self.file_path),
            "title": self.title,
            "content": self.content,
            "content_hash": self.content_hash,
            "embedding": self.embedding,
            "metadata": self.metadata,
            "created_at": self.created_at.isoformat(),
            "updated_at": self.updated_at.isoformat(),
            "chunk_index": self.chunk_index
        }

@dataclass
class SearchResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
    document: VectorDocument
    similarity_score: float
    relevance_context: str
    matched_chunk: Optional[str] = None

# === –ü–†–û–°–¢–ê–Ø –í–ï–ö–¢–û–†–ò–ó–ê–¶–ò–Ø ===

class SimpleEmbedding:
    """–ü—Ä–æ—Å—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"""
    
    def __init__(self):
        self.vocabulary = {}
        self.idf_weights = {}
        self.vocab_size = 0
        
    def build_vocabulary(self, documents: List[str]):
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å —Å–ª–æ–≤–∞—Ä—å –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        word_counts = {}
        doc_count = len(documents)
        
        # –ü–æ–¥—Å—á—ë—Ç —Å–ª–æ–≤
        for doc in documents:
            words = self._tokenize(doc)
            unique_words = set(words)
            
            for word in unique_words:
                word_counts[word] = word_counts.get(word, 0) + 1
        
        # –°–æ–∑–¥–∞—ë–º —Å–ª–æ–≤–∞—Ä—å –∏ IDF –≤–µ—Å–∞
        self.vocabulary = {word: idx for idx, word in enumerate(word_counts.keys())}
        self.vocab_size = len(self.vocabulary)
        
        # –°—á–∏—Ç–∞–µ–º IDF
        for word, count in word_counts.items():
            self.idf_weights[word] = np.log(doc_count / count)
            
        logger.info(f"üß† –°–ª–æ–≤–∞—Ä—å –ø–æ—Å—Ç—Ä–æ–µ–Ω: {self.vocab_size} —Å–ª–æ–≤")
    
    def vectorize(self, text: str) -> List[float]:
        """–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ TF-IDF –≤–µ–∫—Ç–æ—Ä"""
        if not self.vocabulary:
            logger.warning("‚ö†Ô∏è –°–ª–æ–≤–∞—Ä—å –Ω–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω!")
            return [0.0] * 300  # –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω—É–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä
            
        words = self._tokenize(text)
        word_counts = {}
        
        # –ü–æ–¥—Å—á—ë—Ç —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤
        for word in words:
            word_counts[word] = word_counts.get(word, 0) + 1
        
        # –°–æ–∑–¥–∞—ë–º TF-IDF –≤–µ–∫—Ç–æ—Ä
        vector = [0.0] * self.vocab_size
        doc_length = len(words)
        
        for word, count in word_counts.items():
            if word in self.vocabulary:
                idx = self.vocabulary[word]
                tf = count / doc_length  # Term Frequency
                idf = self.idf_weights.get(word, 1.0)  # IDF
                vector[idx] = tf * idf
        
        return vector
    
    def _tokenize(self, text: str) -> List[str]:
        """–ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è"""
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Å–ª–æ–≤–∞–º
        words = text.lower().split()
        
        # –û—á–∏—â–∞–µ–º –æ—Ç –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        clean_words = []
        for word in words:
            clean_word = ''.join(c for c in word if c.isalnum() or c in '—ë–π—Ü—É–∫–µ–Ω–≥—à—â–∑—Ö—ä—Ñ—ã–≤–∞–ø—Ä–æ–ª–¥–∂—ç—è—á—Å–º–∏—Ç—å–±—é')
            if len(clean_word) > 2:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
                clean_words.append(clean_word)
        
        return clean_words
    
    def cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏"""
        if len(vec1) != len(vec2):
            return 0.0
            
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        magnitude1 = sum(a * a for a in vec1) ** 0.5
        magnitude2 = sum(b * b for b in vec2) ** 0.5
        
        if magnitude1 == 0 or magnitude2 == 0:
            return 0.0
            
        return dot_product / (magnitude1 * magnitude2)

# === –í–ï–ö–¢–û–†–ù–û–ï –•–†–ê–ù–ò–õ–ò–©–ï ===

class VectorMemoryStore:
    """–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
    
    def __init__(self, storage_path: str = "vector_memory"):
        self.storage_path = Path(storage_path)
        self.storage_path.mkdir(exist_ok=True)
        
        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã
        self.embedding_model = SimpleEmbedding()
        self.documents: Dict[str, VectorDocument] = {}
        
        # –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è
        self.db_path = self.storage_path / "vector_store.db"
        self._init_database()
        
        # –ö—ç—à –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        self.document_cache = {}
        self.is_indexed = False
        
        logger.info(f"üîç VectorMemoryStore –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {storage_path}")
    
    def _init_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS documents (
                doc_id TEXT PRIMARY KEY,
                file_path TEXT,
                title TEXT,
                content TEXT,
                content_hash TEXT,
                embedding TEXT,
                metadata TEXT,
                created_at TEXT,
                updated_at TEXT,
                chunk_index INTEGER
            )
        """)
        
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_content_hash ON documents(content_hash)
        """)
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_file_path ON documents(file_path)
        """)
        
        conn.commit()
        conn.close()
        
        logger.debug("üìä –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
    
    async def index_documents(self, documents_path: Path) -> int:
        """–ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –ø–∞–ø–∫–∏"""
        indexed_count = 0
        all_texts = []
        
        # –°–Ω–∞—á–∞–ª–∞ —Å–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç—ã –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —Å–ª–æ–≤–∞—Ä—è
        for file_path in documents_path.rglob("*.md"):
            if file_path.is_file():
                try:
                    content = file_path.read_text(encoding='utf-8')
                    all_texts.append(content)
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å {file_path}: {e}")
        
        # –°—Ç—Ä–æ–∏–º —Å–ª–æ–≤–∞—Ä—å
        if all_texts:
            self.embedding_model.build_vocabulary(all_texts)
        
        # –¢–µ–ø–µ—Ä—å –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
        for file_path in documents_path.rglob("*.md"):
            if file_path.is_file():
                if await self._index_document(file_path):
                    indexed_count += 1
        
        self.is_indexed = True
        logger.info(f"‚úÖ –ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ {indexed_count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        return indexed_count
    
    async def _index_document(self, file_path: Path) -> bool:
        """–ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            content = file_path.read_text(encoding='utf-8')
            content_hash = hashlib.md5(content.encode()).hexdigest()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å
            if await self._document_exists(str(file_path), content_hash):
                return False
            
            # –°–æ–∑–¥–∞—ë–º –¥–æ–∫—É–º–µ–Ω—Ç
            doc_id = self._generate_doc_id(file_path, content_hash)
            title = self._extract_title(content, file_path)
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –±–æ–ª—å—à–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ —á–∞—Å—Ç–∏
            chunks = self._split_content(content)
            
            for i, chunk in enumerate(chunks):
                chunk_doc = VectorDocument(
                    doc_id=f"{doc_id}_{i}",
                    file_path=str(file_path),
                    title=title,
                    content=chunk,
                    content_hash=content_hash,
                    chunk_index=i,
                    metadata={
                        "file_name": file_path.name,
                        "file_size": len(content),
                        "chunk_count": len(chunks)
                    }
                )
                
                # –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º
                chunk_doc.embedding = self.embedding_model.vectorize(chunk)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º
                await self._save_document(chunk_doc)
                self.documents[chunk_doc.doc_id] = chunk_doc
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ {file_path}: {e}")
            return False
    
    async def search(self, query: str, limit: int = 5, min_similarity: float = 0.1) -> List[SearchResult]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        if not self.is_indexed:
            await self._load_documents_from_db()
        
        if not self.documents:
            logger.warning("‚ö†Ô∏è –ù–µ—Ç –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            return []
        
        # –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º –∑–∞–ø—Ä–æ—Å
        query_vector = self.embedding_model.vectorize(query)
        
        # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        results = []
        for doc in self.documents.values():
            if doc.embedding:
                similarity = self.embedding_model.cosine_similarity(query_vector, doc.embedding)
                
                if similarity >= min_similarity:
                    # –°–æ–∑–¥–∞—ë–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                    context = self._create_relevance_context(query, doc.content)
                    
                    results.append(SearchResult(
                        document=doc,
                        similarity_score=similarity,
                        relevance_context=context,
                        matched_chunk=self._extract_relevant_chunk(query, doc.content)
                    ))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        results.sort(key=lambda x: x.similarity_score, reverse=True)
        
        logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è '{query[:50]}...'")
        return results[:limit]
    
    def _create_relevance_context(self, query: str, content: str, context_length: int = 200) -> str:
        """–°–æ–∑–¥–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏"""
        query_words = set(self.embedding_model._tokenize(query))
        content_words = self.embedding_model._tokenize(content)
        
        # –ù–∞–π—Ç–∏ –ª—É—á—à–µ–µ –º–µ—Å—Ç–æ –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ
        best_position = 0
        best_score = 0
        
        for i in range(len(content_words) - 10):
            window = content_words[i:i+10]
            score = sum(1 for word in window if word in query_words)
            
            if score > best_score:
                best_score = score
                best_position = i
        
        # –°–æ–∑–¥–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ –ª—É—á—à–µ–≥–æ –º–µ—Å—Ç–∞
        start_char = max(0, content.find(' '.join(content_words[best_position:best_position+1])) - context_length//2)
        end_char = min(len(content), start_char + context_length)
        
        context = content[start_char:end_char].strip()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–Ω–æ–≥–æ—Ç–æ—á–∏–µ –µ—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω
        if start_char > 0:
            context = "..." + context
        if end_char < len(content):
            context = context + "..."
            
        return context
    
    def _extract_relevant_chunk(self, query: str, content: str, chunk_size: int = 300) -> str:
        """–ò–∑–≤–ª–µ—á—å –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫—É—Å–æ–∫ —Ç–µ–∫—Å—Ç–∞"""
        if len(content) <= chunk_size:
            return content
            
        query_words = set(self.embedding_model._tokenize(query))
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –∫—É—Å–∫–∏
        sentences = content.split('.')
        best_chunk = ""
        best_score = 0
        
        for i in range(len(sentences)):
            # –ë–µ—Ä—ë–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
            chunk_sentences = sentences[i:i+3]
            chunk = '. '.join(chunk_sentences)
            
            if len(chunk) > chunk_size:
                chunk = chunk[:chunk_size] + "..."
            
            # –°—á–∏—Ç–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
            chunk_words = set(self.embedding_model._tokenize(chunk))
            score = len(query_words.intersection(chunk_words))
            
            if score > best_score:
                best_score = score
                best_chunk = chunk
        
        return best_chunk or content[:chunk_size]
    
    async def _document_exists(self, file_path: str, content_hash: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å —Ç–∞–∫–∏–º —Ö—ç—à–µ–º"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT 1 FROM documents 
            WHERE file_path = ? AND content_hash = ?
            LIMIT 1
        """, (file_path, content_hash))
        
        exists = cursor.fetchone() is not None
        conn.close()
        
        return exists
    
    async def _save_document(self, document: VectorDocument):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            INSERT OR REPLACE INTO documents 
            (doc_id, file_path, title, content, content_hash, embedding, metadata, created_at, updated_at, chunk_index)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            document.doc_id,
            document.file_path,
            document.title,
            document.content,
            document.content_hash,
            json.dumps(document.embedding),
            json.dumps(document.metadata),
            document.created_at.isoformat(),
            document.updated_at.isoformat(),
            document.chunk_index
        ))
        
        conn.commit()
        conn.close()
    
    async def _load_documents_from_db(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("SELECT * FROM documents")
        rows = cursor.fetchall()
        conn.close()
        
        for row in rows:
            doc_id, file_path, title, content, content_hash, embedding_json, metadata_json, created_at, updated_at, chunk_index = row
            
            document = VectorDocument(
                doc_id=doc_id,
                file_path=file_path,
                title=title,
                content=content,
                content_hash=content_hash,
                embedding=json.loads(embedding_json) if embedding_json else None,
                metadata=json.loads(metadata_json) if metadata_json else {},
                created_at=datetime.fromisoformat(created_at),
                updated_at=datetime.fromisoformat(updated_at),
                chunk_index=chunk_index
            )
            
            self.documents[doc_id] = document
        
        self.is_indexed = len(self.documents) > 0
        logger.info(f"üìö –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –±–∞–∑—ã")
    
    def _generate_doc_id(self, file_path: Path, content_hash: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å ID –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        path_hash = hashlib.md5(str(file_path).encode()).hexdigest()[:8]
        return f"doc_{path_hash}_{content_hash[:8]}"
    
    def _extract_title(self, content: str, file_path: Path) -> str:
        """–ò–∑–≤–ª–µ—á—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ"""
        lines = content.split('\n')
        
        # –ò—â–µ–º –ø–µ—Ä–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ markdown
        for line in lines:
            line = line.strip()
            if line.startswith('# '):
                return line[2:].strip()
            elif line.startswith('## '):
                return line[3:].strip()
        
        # –ï—Å–ª–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
        return file_path.stem
    
    def _split_content(self, content: str, max_chunk_size: int = 1000) -> List[str]:
        """–†–∞–∑–±–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ –∫—É—Å–∫–∏"""
        if len(content) <= max_chunk_size:
            return [content]
        
        chunks = []
        sentences = content.split('.')
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk + sentence) > max_chunk_size:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = sentence
                else:
                    # –ï—Å–ª–∏ –æ–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ
                    chunks.append(sentence[:max_chunk_size])
                    current_chunk = sentence[max_chunk_size:]
            else:
                current_chunk += sentence + "."
        
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return chunks or [content]

# === –ì–õ–û–ë–ê–õ–¨–ù–û–ï –•–†–ê–ù–ò–õ–ò–©–ï ===

_global_vector_store = None

def get_vector_store() -> VectorMemoryStore:
    """–ü–æ–ª—É—á–∏—Ç—å –≥–ª–æ–±–∞–ª—å–Ω–æ–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ"""
    global _global_vector_store
    if _global_vector_store is None:
        _global_vector_store = VectorMemoryStore()
    return _global_vector_store 